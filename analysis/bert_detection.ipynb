{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: torch in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: seqeval in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (4.66.6)\n",
      "Requirement already satisfied: torch in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (2.4.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (1.5.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch->transformers[torch]) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Mobile bert\n",
    "!pip install transformers datasets torch pandas numpy seqeval\n",
    "!pip install tensorflow\n",
    "! pip install tf-keras\n",
    "! pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileBERT Fine-tuning para Detecci贸n de Productos Cosm茅ticos y Atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sandr\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DEPENDENCIAS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from seqeval.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuraci贸n\n",
    "RUTA_DATOS = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\data_bert\\labeled_sentences.xlsx\"\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\products_attributes\"\n",
    "BATCH_SIZE = 9\n",
    "LEARNING_RATE = 3e-5\n",
    "NUM_EPOCHS = 9\n",
    "MAX_LENGTH = 128\n",
    "WARMUP_RATIO = 0.1   \n",
    "SEED = 6\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.2\n",
    "VALIDATION_SPLIT_RATIO = 0.5  # Del conjunto de test, la mitad ser谩 para validaci贸n\n",
    "\n",
    "# Configuraci贸n de etiquetas\n",
    "# Usaremos esquema BIO (Beginning, Inside, Outside) para entidades\n",
    "# B-PRODUCT: Inicio de producto\n",
    "# I-PRODUCT: Continuaci贸n de producto\n",
    "# B-ATTRIBUTE: Inicio de atributo\n",
    "# I-ATTRIBUTE: Continuaci贸n de atributo\n",
    "# O: No es parte de una entidad\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos etiquetados...\n",
      "Datos cargados correctamente. Total de filas: 489\n",
      "Columnas disponibles: id_urlvideo, transcription, products_detected, attributes_detected\n",
      "                                         id_urlvideo  \\\n",
      "0  https://www.tiktok.com/@scurtoworld/video/6868...   \n",
      "1  https://www.tiktok.com/@missprettygirl/video/7...   \n",
      "2  https://www.tiktok.com/@mariane.liebhaber/vide...   \n",
      "3  https://www.tiktok.com/@fake.it.tilu.make.it/v...   \n",
      "4  https://www.tiktok.com/@hunterdestin/video/745...   \n",
      "\n",
      "                                       transcription  \\\n",
      "0  No shadow for me today, but we're gonna have t...   \n",
      "1  Looks like a brown liner and then like a pink ...   \n",
      "2        Next thing are these touch land power mist.   \n",
      "3  And I upgraded to the glass bottles, not the b...   \n",
      "4      So we'll kind of like mix these two together.   \n",
      "\n",
      "                                 products_detected      attributes_detected  \n",
      "0                                eyeliner (makeup)                      NaN  \n",
      "1                   liner (makeup), gloss (makeup)              brown, pink  \n",
      "2                            power mist (skincare)                      NaN  \n",
      "3  glass bottles (packaging), body spray(skincare)  upgrade from body spray  \n",
      "4                                              NaN                  mixable  \n",
      "\n",
      "Preprocesando datos...\n"
     ]
    }
   ],
   "source": [
    "# 1. Carga de datos etiquetados\n",
    "print(\"Cargando datos etiquetados...\")\n",
    "try:\n",
    "    df = pd.read_excel(RUTA_DATOS)\n",
    "    print(f\"Datos cargados correctamente. Total de filas: {len(df)}\")\n",
    "    print(f\"Columnas disponibles: {', '.join(df.columns)}\")\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar los datos: {str(e)}\")\n",
    "\n",
    "print(\"\\nPreprocesando datos...\")\n",
    "\n",
    "# Funci贸n para dividir texto en tokens a nivel de palabra\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Tokeniza un texto en palabras.\"\"\"\n",
    "    # Manejar NaN o valores vac铆os\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return []\n",
    "    # Tokenizaci贸n simple por espacios y puntuaci贸n\n",
    "    tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
    "    return tokens\n",
    "\n",
    "# Funci贸n para convertir etiquetas de productos y atributos a formato BIO\n",
    "def create_bio_tags(text, products, attributes):\n",
    "    \"\"\"\n",
    "    Crea etiquetas BIO para un texto dado productos y atributos.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto original\n",
    "        products (str): Lista de productos separados por comas\n",
    "        attributes (str): Lista de atributos separados por comas\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de etiquetas BIO alineadas con tokens del texto\n",
    "    \"\"\"\n",
    "    tokens = simple_tokenize(text)\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    # Inicializar todas las etiquetas como \"O\" (Outside)\n",
    "    bio_tags = [\"O\"] * len(tokens)\n",
    "    \n",
    "    # Procesar productos\n",
    "    if not pd.isna(products) and products.strip() != \"\":\n",
    "        product_list = [p.strip().lower() for p in products.split(',')]\n",
    "        for product in product_list:\n",
    "            # Extraer el nombre del producto sin la categor铆a entre par茅ntesis\n",
    "            match = re.match(r\"(.*?)(\\s*\\(.*\\))?$\", product)\n",
    "            if match:\n",
    "                product_name = match.group(1).strip().lower()\n",
    "                product_tokens = simple_tokenize(product_name)\n",
    "                \n",
    "                if product_tokens:\n",
    "                    # Buscar el producto en el texto tokenizado\n",
    "                    for i in range(len(tokens) - len(product_tokens) + 1):\n",
    "                        if [t.lower() for t in tokens[i:i+len(product_tokens)]] == [t.lower() for t in product_tokens]:\n",
    "                            # Marcar el primer token como B-PRODUCT\n",
    "                            bio_tags[i] = \"B-PRODUCT\"\n",
    "                            # Marcar los tokens restantes como I-PRODUCT\n",
    "                            for j in range(1, len(product_tokens)):\n",
    "                                bio_tags[i + j] = \"I-PRODUCT\"\n",
    "    \n",
    "    # Procesar atributos\n",
    "    if not pd.isna(attributes) and attributes.strip() != \"\":\n",
    "        attribute_list = [a.strip().lower() for a in attributes.split(',')]\n",
    "        for attribute in attribute_list:\n",
    "            attribute_tokens = simple_tokenize(attribute)\n",
    "            \n",
    "            if attribute_tokens:\n",
    "                # Buscar el atributo en el texto tokenizado\n",
    "                for i in range(len(tokens) - len(attribute_tokens) + 1):\n",
    "                    if [t.lower() for t in tokens[i:i+len(attribute_tokens)]] == [t.lower() for t in attribute_tokens]:\n",
    "                        # Marcar el primer token como B-ATTRIBUTE\n",
    "                        bio_tags[i] = \"B-ATTRIBUTE\"\n",
    "                        # Marcar los tokens restantes como I-ATTRIBUTE\n",
    "                        for j in range(1, len(attribute_tokens)):\n",
    "                            bio_tags[i + j] = \"I-ATTRIBUTE\"\n",
    "    \n",
    "    return list(zip(tokens, bio_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223bd9c3cb00495a9c28cdeb5d902624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creando etiquetas BIO:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesamiento completado. Datos tokenizados: 489. Filas omitidas: 0\n"
     ]
    }
   ],
   "source": [
    "# Aplicar el procesamiento a nuestros datos\n",
    "tokenized_data = []\n",
    "skipped_rows = 0\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Creando etiquetas BIO\"):\n",
    "    try:\n",
    "        text = row['transcription']\n",
    "        products = row['products_detected'] if 'products_detected' in row else \"\"\n",
    "        attributes = row['attributes_detected'] if 'attributes_detected' in row else \"\"\n",
    "        \n",
    "        if pd.isna(text) or text.strip() == \"\":\n",
    "            skipped_rows += 1\n",
    "            continue\n",
    "            \n",
    "        token_tag_pairs = create_bio_tags(text, products, attributes)\n",
    "        \n",
    "        if token_tag_pairs:\n",
    "            tokenized_data.append({\n",
    "                'id': i,\n",
    "                'tokens': [pair[0] for pair in token_tag_pairs],\n",
    "                'bio_tags': [pair[1] for pair in token_tag_pairs],\n",
    "                'text': text,\n",
    "                'products': products if not pd.isna(products) else \"\",\n",
    "                'attributes': attributes if not pd.isna(attributes) else \"\"\n",
    "            })\n",
    "        else:\n",
    "            skipped_rows += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la fila {i}: {str(e)}\")\n",
    "        skipped_rows += 1\n",
    "\n",
    "print(f\"Procesamiento completado. Datos tokenizados: {len(tokenized_data)}. Filas omitidas: {skipped_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribuci贸n de etiquetas:\n",
      "I-ATTRIBUTE: 202 (2.63%)\n",
      "I-PRODUCT: 114 (1.48%)\n",
      "B-ATTRIBUTE: 380 (4.94%)\n",
      "O: 6638 (86.26%)\n",
      "B-PRODUCT: 361 (4.69%)\n"
     ]
    }
   ],
   "source": [
    "# Revisar estad铆sticas de etiquetas\n",
    "all_tags = [tag for item in tokenized_data for tag in item['bio_tags']]\n",
    "tag_counts = {tag: all_tags.count(tag) for tag in set(all_tags)}\n",
    "print(\"\\nDistribuci贸n de etiquetas:\")\n",
    "for tag, count in tag_counts.items():\n",
    "    print(f\"{tag}: {count} ({count/len(all_tags)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuci贸n de Etiquetas:\n",
    "- **O (Outside)**: 94.93%  \n",
    "- **B-PRODUCT (Inicio de productos)**: 1.88%  \n",
    "- **I-PRODUCT (Continuaci贸n de productos)**: 0.38%  \n",
    "- **B-ATTRIBUTE (Inicio de atributos)**: 2.27%  \n",
    "- **I-ATTRIBUTE (Continuaci贸n de atributos)**: 0.55%  \n",
    "\n",
    "Este tipo de desequilibrio es bastante com煤n en tareas de Named Entity Recognition (NER) porque la mayor铆a de las palabras en un texto no suelen ser entidades nombradas. Sin embargo, podr铆a afectar el rendimiento del modelo, ya que hay pocas muestras de las clases minoritarias.  \n",
    "\n",
    "#### Para Abordar el Desequilibrio:\n",
    "\n",
    "- Sobremuestreo de Clases Minoritarias: Etiquetar m谩s ejemplos que contengan productos y atributos para aumentar su representaci贸n en los datos de entrenamiento.  \n",
    "-  Ajustar los Pesos de las Clases: Se puede modificar el c贸digo para dar m谩s peso a las clases minoritarias durante el entrenamiento:  \n",
    "```python\n",
    "# En la definici贸n de TrainingArguments\n",
    "class_weights = torch.tensor([1.0, 20.0, 30.0, 20.0, 30.0])  # Ajustar estos valores seg煤n necesidades\n",
    "loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "```\n",
    "- Regularizaci贸n M谩s Fuerte: Para evitar que el modelo simplemente prediga \"O\" para todo, se puede aumentar la penalizaci贸n por sobreajuste:  \n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    # Otros par谩metros...\n",
    "    weight_decay=0.1  # Aumentar el valor original (por ejemplo, de 0.01 a 0.1)\n",
    ")\n",
    "```\n",
    "- T茅cnicas de Data Augmentation: Generar m谩s ejemplos sint茅ticos para aumentar la representaci贸n de productos y atributos en el conjunto de entrenamiento.  \n",
    "- Ajustar el N煤mero de pocas: Considerar aumentar el n煤mero de 茅pocas de entrenamiento para que el modelo tenga m谩s oportunidades de aprender los patrones menos frecuentes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dividiendo datos en conjuntos de entrenamiento, validaci贸n y prueba...\n",
      "Conjunto de entrenamiento: 391 ejemplos\n",
      "Conjunto de validaci贸n: 49 ejemplos\n",
      "Conjunto de prueba: 49 ejemplos\n"
     ]
    }
   ],
   "source": [
    "# 3. Dividir en conjuntos de entrenamiento, validaci贸n y prueba\n",
    "print(\"\\nDividiendo datos en conjuntos de entrenamiento, validaci贸n y prueba...\")\n",
    "\n",
    "train_data, test_data = train_test_split(\n",
    "    tokenized_data, \n",
    "    test_size=TRAIN_TEST_SPLIT_RATIO, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "val_data, test_data = train_test_split(\n",
    "    test_data, \n",
    "    test_size=VALIDATION_SPLIT_RATIO, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Conjunto de entrenamiento: {len(train_data)} ejemplos\")\n",
    "print(f\"Conjunto de validaci贸n: {len(val_data)} ejemplos\")\n",
    "print(f\"Conjunto de prueba: {len(test_data)} ejemplos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparando datos para MobileBERT...\n",
      "Mapeo de etiquetas: {'B-ATTRIBUTE': 0, 'B-PRODUCT': 1, 'I-ATTRIBUTE': 2, 'I-PRODUCT': 3, 'O': 4}\n"
     ]
    }
   ],
   "source": [
    "# 4. Preparaci贸n para MobileBERT\n",
    "print(\"\\nPreparando datos para MobileBERT...\")\n",
    "\n",
    "# Cargar tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Definir mapeo de etiquetas a IDs\n",
    "unique_tags = sorted(list(set(all_tags)))\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "\n",
    "# Guardar el mapeo para uso futuro\n",
    "with open(os.path.join(OUTPUT_DIR, 'tag_mapping.json'), 'w') as f:\n",
    "    json.dump({'tag2id': tag2id, 'id2tag': id2tag}, f)\n",
    "\n",
    "print(f\"Mapeo de etiquetas: {tag2id}\")\n",
    "\n",
    "# Clase para tokenizar y preparar los datos para MobileBERT\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        tokens = item['tokens']\n",
    "        bio_tags = item['bio_tags']\n",
    "        \n",
    "        # Tokenizar palabras usando el tokenizador de MobileBERT\n",
    "        word_ids = []\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        labels = []\n",
    "        \n",
    "        # A帽adir token [CLS] al inicio\n",
    "        input_ids.append(tokenizer.cls_token_id)\n",
    "        attention_mask.append(1)\n",
    "        word_ids.append(None)\n",
    "        labels.append(-100)  # -100 es ignorado en la funci贸n de p茅rdida\n",
    "        \n",
    "        # Tokenizar cada palabra y alinear con etiquetas BIO\n",
    "        for word_idx, (word, tag) in enumerate(zip(tokens, bio_tags)):\n",
    "            # Tokenizar la palabra (podr铆a dar m煤ltiples tokens)\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                # Si la palabra est谩 fuera del vocabulario o es ignorada, usamos el token desconocido\n",
    "                word_tokens = [tokenizer.unk_token]\n",
    "                \n",
    "            # A帽adir IDs de tokens\n",
    "            for i, _ in enumerate(word_tokens):\n",
    "                input_ids.append(tokenizer.convert_tokens_to_ids(word_tokens[i]))\n",
    "                attention_mask.append(1)\n",
    "                word_ids.append(word_idx)\n",
    "                \n",
    "                # Solo el primer subtoken mantiene la etiqueta, el resto se ignora\n",
    "                if i == 0:\n",
    "                    labels.append(tag2id[tag])\n",
    "                else:\n",
    "                    labels.append(-100)\n",
    "        \n",
    "        # A帽adir token [SEP] al final\n",
    "        input_ids.append(tokenizer.sep_token_id)\n",
    "        attention_mask.append(1)\n",
    "        word_ids.append(None)\n",
    "        labels.append(-100)\n",
    "        \n",
    "        # Rellenar o truncar a max_length\n",
    "        padding_length = self.max_length - len(input_ids)\n",
    "        \n",
    "        if padding_length > 0:\n",
    "            # Rellenar\n",
    "            input_ids.extend([tokenizer.pad_token_id] * padding_length)\n",
    "            attention_mask.extend([0] * padding_length)\n",
    "            labels.extend([-100] * padding_length)\n",
    "            word_ids.extend([None] * padding_length)\n",
    "        elif padding_length < 0:\n",
    "            # Truncar\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "            attention_mask = attention_mask[:self.max_length]\n",
    "            labels = labels[:self.max_length]\n",
    "            word_ids = word_ids[:self.max_length]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'word_ids': word_ids  # Esto es para debugging, no se usa en entrenamiento\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurando y entrenando el modelo MobileBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sandr\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\sandr\\AppData\\Local\\Temp\\ipykernel_12088\\2872197540.py:127: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Crear conjuntos de datos\n",
    "train_dataset = NERDataset(train_data, tokenizer, MAX_LENGTH)\n",
    "val_dataset = NERDataset(val_data, tokenizer, MAX_LENGTH)\n",
    "test_dataset = NERDataset(test_data, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# 5. Configurar y entrenar el modelo\n",
    "print(\"\\nConfigurando y entrenando el modelo MobileBERT...\")\n",
    "\n",
    "# Aumentar significativamente los pesos para las clases minoritarias\n",
    "total_labels = 5450 + 169 + 132 + 44 + 28  # Total de todas las etiquetas\n",
    "\n",
    "# Opci贸n m谩s agresiva para los pesos\n",
    "class_weights = [\n",
    "    0.3,                         # O (casi ignorado)\n",
    "    total_labels / 143 * 3.0,     # B-PRODUCT (peso extremadamente alto)\n",
    "    total_labels / 28 * 3.0,     # I-PRODUCT (peso extremadamente alto)\n",
    "    total_labels / 194 * 9.0,     # B-ATTRIBUTE (peso extremadamente alto)\n",
    "    total_labels / 38 * 9.0      # I-ATTRIBUTE (peso extremadamente alto)\n",
    "]\n",
    "# Limitar los pesos m谩ximos, pero permitir valores m谩s altos\n",
    "class_weights = [min(weight, 50.0) for weight in class_weights]\n",
    "\n",
    "# Cargar modelo base\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(unique_tags),\n",
    "    id2label=id2tag,\n",
    "    label2id=tag2id\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Configurar argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.03,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",  # Cambiado a F1 para enfocarse en clases minoritarias\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, 'logs'),\n",
    ")\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "# Definir data collator personalizado con pesos de clase\n",
    "class WeightedDataCollator(DataCollatorForTokenClassification):\n",
    "    def __init__(self, tokenizer, class_weights, label_name=\"labels\"):\n",
    "        super().__init__(tokenizer=tokenizer)\n",
    "        self.class_weights = torch.tensor(class_weights)\n",
    "        self.label_name = label_name\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch = super().__call__(features)\n",
    "        return batch\n",
    "\n",
    "# M茅trica personalizada para evaluaci贸n que incluye F1\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Eliminar tokens especiales y padding\n",
    "    true_predictions = [\n",
    "        [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Calcular m茅tricas usando seqeval\n",
    "    report = classification_report(true_labels, true_predictions, output_dict=True)\n",
    "    \n",
    "    # Formatear para el trainer\n",
    "    results = {\n",
    "        'accuracy': report['micro avg']['precision'],\n",
    "        'precision': report['macro avg']['precision'],\n",
    "        'recall': report['macro avg']['recall'],\n",
    "        'f1': report['macro avg']['f1-score']\n",
    "    }\n",
    "    \n",
    "    # A帽adir m茅tricas por clase\n",
    "    for key in report:\n",
    "        if key not in ['macro avg', 'micro avg', 'weighted avg']:\n",
    "            results[f\"{key}_f1\"] = report[key]['f1-score']\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Inicializar trainer con loss_fct personalizado\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Tensor de pesos\n",
    "        weight = torch.tensor(class_weights, device=logits.device)\n",
    "        \n",
    "        # Calcular probabilidades\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        logits_view = logits.view(-1, model.config.num_labels)\n",
    "        labels_view = labels.view(-1)\n",
    "        \n",
    "        # Seleccionar probabilidades para las clases correctas\n",
    "        mask = labels_view != -100\n",
    "        valid_labels = labels_view[mask]\n",
    "        valid_logits = logits_view[mask]\n",
    "        \n",
    "        # Focal loss manual\n",
    "        gamma = 1.5\n",
    "        ce_loss = torch.nn.CrossEntropyLoss(weight=weight, reduction='none')\n",
    "        ce = ce_loss(valid_logits, valid_labels)\n",
    "        \n",
    "        pt = torch.exp(-ce)\n",
    "        focal_loss = ((1 - pt) ** gamma) * ce\n",
    "        \n",
    "        return (focal_loss.mean(), outputs) if return_outputs else focal_loss.mean()\n",
    "\n",
    "# Usar el CustomTrainer en lugar del Trainer est谩ndar\n",
    "# Inicializar trainer con loss_fct personalizado\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp\\ipykernel_12088\\3984830881.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando entrenamiento...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27744e183fed4adabbab50f00d61d646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c7e6384bc741ba86a178f0b1989a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3114899694919586, 'eval_accuracy': 0.417910447761194, 'eval_precision': 0.3819047619047619, 'eval_recall': 0.358678955453149, 'eval_f1': 0.36904761904761907, 'eval_ATTRIBUTE_f1': 0.21428571428571427, 'eval_ATTRIBUTE_precision': 0.24, 'eval_ATTRIBUTE_recall': 0.1935483870967742, 'eval_PRODUCT_f1': 0.5238095238095238, 'eval_PRODUCT_precision': 0.5238095238095238, 'eval_PRODUCT_recall': 0.5238095238095238, 'eval_runtime': 2.9585, 'eval_samples_per_second': 16.563, 'eval_steps_per_second': 2.028, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64befbf246fc4b7ca7db8f74ba239178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21792076528072357, 'eval_accuracy': 0.5633802816901409, 'eval_precision': 0.5399193548387097, 'eval_recall': 0.5226574500768049, 'eval_f1': 0.5310778914240756, 'eval_ATTRIBUTE_f1': 0.35483870967741943, 'eval_ATTRIBUTE_precision': 0.3548387096774194, 'eval_ATTRIBUTE_recall': 0.3548387096774194, 'eval_PRODUCT_f1': 0.7073170731707318, 'eval_PRODUCT_precision': 0.725, 'eval_PRODUCT_recall': 0.6904761904761905, 'eval_runtime': 3.2509, 'eval_samples_per_second': 15.073, 'eval_steps_per_second': 1.846, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc142ea2b81448c95477e5097bbf21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20430731773376465, 'eval_accuracy': 0.573170731707317, 'eval_precision': 0.562015503875969, 'eval_recall': 0.6144393241167435, 'eval_f1': 0.5857142857142857, 'eval_ATTRIBUTE_f1': 0.3714285714285714, 'eval_ATTRIBUTE_precision': 0.3333333333333333, 'eval_ATTRIBUTE_recall': 0.41935483870967744, 'eval_PRODUCT_f1': 0.8, 'eval_PRODUCT_precision': 0.7906976744186046, 'eval_PRODUCT_recall': 0.8095238095238095, 'eval_runtime': 3.3366, 'eval_samples_per_second': 14.686, 'eval_steps_per_second': 1.798, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3e24fa3ed5428a890dfdb85f1facd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2287573516368866, 'eval_accuracy': 0.5949367088607594, 'eval_precision': 0.6188630490956072, 'eval_recall': 0.6228878648233487, 'eval_f1': 0.612959112959113, 'eval_ATTRIBUTE_f1': 0.4054054054054055, 'eval_ATTRIBUTE_precision': 0.3488372093023256, 'eval_ATTRIBUTE_recall': 0.4838709677419355, 'eval_PRODUCT_f1': 0.8205128205128205, 'eval_PRODUCT_precision': 0.8888888888888888, 'eval_PRODUCT_recall': 0.7619047619047619, 'eval_runtime': 3.2092, 'eval_samples_per_second': 15.269, 'eval_steps_per_second': 1.87, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b89a3ec8fa74f3bb5bf801a55adacd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23773092031478882, 'eval_accuracy': 0.6025641025641025, 'eval_precision': 0.625, 'eval_recall': 0.618663594470046, 'eval_f1': 0.6148577449947312, 'eval_ATTRIBUTE_f1': 0.3835616438356164, 'eval_ATTRIBUTE_precision': 0.3333333333333333, 'eval_ATTRIBUTE_recall': 0.45161290322580644, 'eval_PRODUCT_f1': 0.8461538461538461, 'eval_PRODUCT_precision': 0.9166666666666666, 'eval_PRODUCT_recall': 0.7857142857142857, 'eval_runtime': 3.0671, 'eval_samples_per_second': 15.976, 'eval_steps_per_second': 1.956, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea2a7786aea4bb99daffb5c2a59ac5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2715701758861542, 'eval_accuracy': 0.703125, 'eval_precision': 0.6581581581581581, 'eval_recall': 0.5779569892473119, 'eval_f1': 0.6154517677869926, 'eval_ATTRIBUTE_f1': 0.3448275862068965, 'eval_ATTRIBUTE_precision': 0.37037037037037035, 'eval_ATTRIBUTE_recall': 0.3225806451612903, 'eval_PRODUCT_f1': 0.8860759493670887, 'eval_PRODUCT_precision': 0.9459459459459459, 'eval_PRODUCT_recall': 0.8333333333333334, 'eval_runtime': 6.6943, 'eval_samples_per_second': 7.32, 'eval_steps_per_second': 0.896, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69815d02b028449f8ac7767be53a72bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2751080393791199, 'eval_accuracy': 0.5833333333333334, 'eval_precision': 0.5833333333333334, 'eval_recall': 0.6424731182795699, 'eval_f1': 0.6084474885844748, 'eval_ATTRIBUTE_f1': 0.3835616438356164, 'eval_ATTRIBUTE_precision': 0.3333333333333333, 'eval_ATTRIBUTE_recall': 0.45161290322580644, 'eval_PRODUCT_f1': 0.8333333333333334, 'eval_PRODUCT_precision': 0.8333333333333334, 'eval_PRODUCT_recall': 0.8333333333333334, 'eval_runtime': 3.5154, 'eval_samples_per_second': 13.939, 'eval_steps_per_second': 1.707, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8e08b06dc94bd0a37813f33d4e645b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27534908056259155, 'eval_accuracy': 0.6025641025641025, 'eval_precision': 0.6025641025641025, 'eval_recall': 0.618663594470046, 'eval_f1': 0.6074074074074074, 'eval_ATTRIBUTE_f1': 0.4, 'eval_ATTRIBUTE_precision': 0.358974358974359, 'eval_ATTRIBUTE_recall': 0.45161290322580644, 'eval_PRODUCT_f1': 0.8148148148148148, 'eval_PRODUCT_precision': 0.8461538461538461, 'eval_PRODUCT_recall': 0.7857142857142857, 'eval_runtime': 3.2505, 'eval_samples_per_second': 15.075, 'eval_steps_per_second': 1.846, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21f2ebf42f04508a1c82e3e8f3a7c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28308719396591187, 'eval_accuracy': 0.6052631578947368, 'eval_precision': 0.5987525987525988, 'eval_recall': 0.6025345622119815, 'eval_f1': 0.5985838779956427, 'eval_ATTRIBUTE_f1': 0.38235294117647056, 'eval_ATTRIBUTE_precision': 0.35135135135135137, 'eval_ATTRIBUTE_recall': 0.41935483870967744, 'eval_PRODUCT_f1': 0.8148148148148148, 'eval_PRODUCT_precision': 0.8461538461538461, 'eval_PRODUCT_recall': 0.7857142857142857, 'eval_runtime': 3.0294, 'eval_samples_per_second': 16.175, 'eval_steps_per_second': 1.981, 'epoch': 9.0}\n",
      "{'train_runtime': 986.9814, 'train_samples_per_second': 3.565, 'train_steps_per_second': 0.401, 'train_loss': 0.1339192245945786, 'epoch': 9.0}\n",
      "Modelo guardado en: C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\products_attributes\\final_model\n",
      "\n",
      "Evaluando el modelo en el conjunto de prueba...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1be58d8991e438d8fa30130f4e0f01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados de evaluaci贸n:\n",
      "eval_loss: 0.2685\n",
      "eval_accuracy: 0.6778\n",
      "eval_precision: 0.7176\n",
      "eval_recall: 0.7147\n",
      "eval_f1: 0.7145\n",
      "eval_ATTRIBUTE_f1: 0.5490\n",
      "eval_ATTRIBUTE_precision: 0.5185\n",
      "eval_ATTRIBUTE_recall: 0.5833\n",
      "eval_PRODUCT_f1: 0.8800\n",
      "eval_PRODUCT_precision: 0.9167\n",
      "eval_PRODUCT_recall: 0.8462\n",
      "eval_runtime: 3.0659\n",
      "eval_samples_per_second: 15.9820\n",
      "eval_steps_per_second: 1.9570\n",
      "epoch: 9.0000\n"
     ]
    }
   ],
   "source": [
    "# M茅trica personalizada para evaluaci贸n\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Eliminar tokens especiales y padding\n",
    "    true_predictions = [\n",
    "        [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Calcular m茅tricas usando seqeval\n",
    "    report = classification_report(true_labels, true_predictions, output_dict=True)\n",
    "    \n",
    "    # Formatear para el trainer\n",
    "    results = {\n",
    "        'accuracy': report['micro avg']['precision'],\n",
    "        'precision': report['macro avg']['precision'],\n",
    "        'recall': report['macro avg']['recall'],\n",
    "        'f1': report['macro avg']['f1-score']\n",
    "    }\n",
    "    \n",
    "    # A帽adir m茅tricas por clase\n",
    "    for key in report:\n",
    "        if key not in ['macro avg', 'micro avg', 'weighted avg']:\n",
    "            results[f\"{key}_f1\"] = report[key]['f1-score']\n",
    "            results[f\"{key}_precision\"] = report[key]['precision']\n",
    "            results[f\"{key}_recall\"] = report[key]['recall']\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Inicializar el entrenador\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "print(\"Comenzando entrenamiento...\")\n",
    "trainer.train()\n",
    "\n",
    "# Guardar el modelo final\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, 'final_model'))\n",
    "print(f\"Modelo guardado en: {os.path.join(OUTPUT_DIR, 'final_model')}\")\n",
    "\n",
    "# 6. Evaluaci贸n final\n",
    "print(\"\\nEvaluando el modelo en el conjunto de prueba...\")\n",
    "results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# Guardar resultados\n",
    "with open(os.path.join(OUTPUT_DIR, 'evaluation_results.json'), 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"\\nResultados de evaluaci贸n:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Funci贸n para extraer productos y atributos de texto\n",
    "def predict_entities(text, model, tokenizer, tag2id, id2tag, max_length=128):\n",
    "    \"\"\"\n",
    "    Predice entidades (productos y atributos) en un texto usando el modelo entrenado.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a analizar\n",
    "        model: Modelo MobileBERT entrenado\n",
    "        tokenizer: Tokenizador MobileBERT\n",
    "        tag2id: Mapeo de etiquetas a IDs\n",
    "        id2tag: Mapeo de IDs a etiquetas\n",
    "        max_length: Longitud m谩xima de secuencia\n",
    "        \n",
    "    Returns:\n",
    "        dict: Diccionario con productos y atributos detectados\n",
    "    \"\"\"\n",
    "    # Tokenizar el texto\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    \n",
    "    # Hacer la predicci贸n\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded)\n",
    "    \n",
    "    # Obtener etiquetas predichas\n",
    "    predictions = torch.argmax(outputs.logits, axis=2).squeeze().tolist()\n",
    "    \n",
    "    # Obtener tokens originales\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "    \n",
    "    # Reconstruir entidades\n",
    "    products = []\n",
    "    attributes = []\n",
    "    \n",
    "    current_entity = []\n",
    "    current_type = None\n",
    "    \n",
    "    # Procesamos token por token\n",
    "    for token, pred_id in zip(tokens, predictions):\n",
    "        # Ignorar tokens especiales y continuaciones de subwords\n",
    "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token] or token.startswith(\"##\"):\n",
    "            if current_entity and current_type:\n",
    "                entity_text = \" \".join(current_entity).replace(\" ##\", \"\")\n",
    "                if current_type == \"PRODUCT\":\n",
    "                    products.append(entity_text)\n",
    "                elif current_type == \"ATTRIBUTE\":\n",
    "                    attributes.append(entity_text)\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "            continue\n",
    "        \n",
    "        # Obtener etiqueta predicha\n",
    "        pred_tag = id2tag.get(pred_id, \"O\")\n",
    "        \n",
    "        # Procesar seg煤n etiqueta\n",
    "        if pred_tag.startswith(\"B-\"):\n",
    "            # Si ya ten铆amos una entidad en curso, la guardamos\n",
    "            if current_entity and current_type:\n",
    "                entity_text = \" \".join(current_entity).replace(\" ##\", \"\")\n",
    "                if current_type == \"PRODUCT\":\n",
    "                    products.append(entity_text)\n",
    "                elif current_type == \"ATTRIBUTE\":\n",
    "                    attributes.append(entity_text)\n",
    "            \n",
    "            # Comenzar nueva entidad\n",
    "            current_entity = [token]\n",
    "            current_type = pred_tag[2:]  # Quitar el \"B-\"\n",
    "            \n",
    "        elif pred_tag.startswith(\"I-\") and current_entity:\n",
    "            # Asegurarnos de que estamos continuando el mismo tipo de entidad\n",
    "            if pred_tag[2:] == current_type:\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "                # Si cambia el tipo, guardamos la anterior y comenzamos una nueva\n",
    "                entity_text = \" \".join(current_entity).replace(\" ##\", \"\")\n",
    "                if current_type == \"PRODUCT\":\n",
    "                    products.append(entity_text)\n",
    "                elif current_type == \"ATTRIBUTE\":\n",
    "                    attributes.append(entity_text)\n",
    "                \n",
    "                current_entity = [token]\n",
    "                current_type = pred_tag[2:]\n",
    "                \n",
    "        elif pred_tag == \"O\" and current_entity:\n",
    "            # Fin de entidad\n",
    "            entity_text = \" \".join(current_entity).replace(\" ##\", \"\")\n",
    "            if current_type == \"PRODUCT\":\n",
    "                products.append(entity_text)\n",
    "            elif current_type == \"ATTRIBUTE\":\n",
    "                attributes.append(entity_text)\n",
    "            \n",
    "            current_entity = []\n",
    "            current_type = None\n",
    "    \n",
    "    # No olvidar la 煤ltima entidad si qued贸 alguna\n",
    "    if current_entity and current_type:\n",
    "        entity_text = \" \".join(current_entity).replace(\" ##\", \"\")\n",
    "        if current_type == \"PRODUCT\":\n",
    "            products.append(entity_text)\n",
    "        elif current_type == \"ATTRIBUTE\":\n",
    "            attributes.append(entity_text)\n",
    "    \n",
    "    return {\n",
    "        \"products\": list(set(products)),  # Eliminar duplicados\n",
    "        \"attributes\": list(set(attributes))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo de uso del modelo:\n",
      "\n",
      "Texto: I love this foundation because it gives me a natural finish and doesn't crease all day.\n",
      "Productos detectados: ['foundation']\n",
      "Atributos detectados: ['natural finish']\n",
      "\n",
      "Texto: The concealer is great for dark circles and has amazing coverage.\n",
      "Productos detectados: ['conceal']\n",
      "Atributos detectados: []\n",
      "\n",
      "Texto: This highlighter gives me a beautiful glow and lasts all day.\n",
      "Productos detectados: ['highlight']\n",
      "Atributos detectados: ['glow']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Ejemplo de uso\n",
    "print(\"\\nEjemplo de uso del modelo:\")\n",
    "\n",
    "# Cargar modelo y mapeo de etiquetas\n",
    "model_path = os.path.join(OUTPUT_DIR, 'final_model')\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'tag_mapping.json'), 'r') as f:\n",
    "    tag_mapping = json.load(f)\n",
    "    tag2id = tag_mapping['tag2id']\n",
    "    id2tag = {int(k): v for k, v in tag_mapping['id2tag'].items()}\n",
    "\n",
    "# Probar con algunos ejemplos\n",
    "test_examples = [\n",
    "    \"I love this foundation because it gives me a natural finish and doesn't crease all day.\",\n",
    "    \"The concealer is great for dark circles and has amazing coverage.\",\n",
    "    \"This highlighter gives me a beautiful glow and lasts all day.\"\n",
    "]\n",
    "\n",
    "for example in test_examples:\n",
    "    results = predict_entities(example, model, tokenizer, tag2id, id2tag)\n",
    "    print(f\"\\nTexto: {example}\")\n",
    "    print(f\"Productos detectados: {results['products']}\")\n",
    "    print(f\"Atributos detectados: {results['attributes']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo completo: C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\results_brand_detection.xlsx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2da0c8f2f7546be861557a64b048087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando transcripciones:   0%|          | 0/6781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset procesado guardado en: C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\products_attributes\\results\\sentences_transcriptions_processed.xlsx\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 9. C贸digo para procesar todo el dataset original\n",
    "def process_full_dataset(input_file, output_file, model, tokenizer, tag2id, id2tag):\n",
    "    \"\"\"\n",
    "    Procesa todo el dataset original y genera un nuevo archivo con productos y atributos detectados.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Ruta al archivo Excel original\n",
    "        output_file (str): Ruta donde guardar el archivo procesado\n",
    "        model: Modelo MobileBERT entrenado\n",
    "        tokenizer: Tokenizador MobileBERT\n",
    "        tag2id: Mapeo de etiquetas a IDs\n",
    "        id2tag: Mapeo de IDs a etiquetas\n",
    "    \"\"\"\n",
    "    print(f\"Procesando archivo completo: {input_file}\")\n",
    "    \n",
    "    # Cargar el dataset original\n",
    "    df_original = pd.read_excel(input_file)\n",
    "    \n",
    "    # Columnas para resultados\n",
    "    df_original['detected_products'] = \"\"\n",
    "    df_original['detected_attributes'] = \"\"\n",
    "    \n",
    "    # Procesar cada fila\n",
    "    for i, row in tqdm(df_original.iterrows(), total=len(df_original), desc=\"Procesando transcripciones\"):\n",
    "        try:\n",
    "            text = row['transcription']\n",
    "            if pd.isna(text) or text.strip() == \"\":\n",
    "                continue\n",
    "                \n",
    "            # Predecir entidades\n",
    "            results = predict_entities(text, model, tokenizer, tag2id, id2tag)\n",
    "            \n",
    "            # Guardar resultados\n",
    "            df_original.at[i, 'detected_products'] = \", \".join(results['products'])\n",
    "            df_original.at[i, 'detected_attributes'] = \", \".join(results['attributes'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando fila {i}: {str(e)}\")\n",
    "    \n",
    "    # Guardar el dataset procesado\n",
    "    df_original.to_excel(output_file, index=False)\n",
    "    print(f\"Dataset procesado guardado en: {output_file}\")\n",
    "\n",
    "# Ejemplo de uso para procesar todo el dataset\n",
    "# Descomenta estas l铆neas cuando quieras procesar todo el archivo\n",
    "full_dataset_path = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\results_brand_detection.xlsx\"\n",
    "output_path = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\products_attributes\\results\\sentences_transcriptions_processed.xlsx\"\n",
    "process_full_dataset(full_dataset_path, output_path, model, tokenizer, tag2id, id2tag)\n",
    "\n",
    "\n",
    "# 10. Visualizaci贸n de p茅rdida durante entrenamiento\n",
    "train_history = trainer.state.log_history\n",
    "\n",
    "# Extraer valores de p茅rdida\n",
    "train_losses = [x.get('loss') for x in train_history if 'loss' in x and 'eval_loss' not in x]\n",
    "train_steps = [x.get('step') for x in train_history if 'loss' in x and 'eval_loss' not in x]\n",
    "eval_losses = [x.get('eval_loss') for x in train_history if 'eval_loss' in x]\n",
    "eval_steps = [x.get('step') for x in train_history if 'eval_loss' in x]\n",
    "\n",
    "# Graficar p茅rdida\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_steps, train_losses, label='Training Loss')\n",
    "plt.plot(eval_steps, eval_losses, label='Validation Loss', marker='o')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'loss_curves.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado en: C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\detection_results\\results_detections.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_urlvideo</th>\n",
       "      <th>transcription</th>\n",
       "      <th>brand_detected</th>\n",
       "      <th>match_text</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>context_text</th>\n",
       "      <th>detected_products</th>\n",
       "      <th>detected_attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.tiktok.com/@aliciarev/video/728025...</td>\n",
       "      <td>I use the Genifit from Lancome, which costs on...</td>\n",
       "      <td>Lanc么me</td>\n",
       "      <td>e Genifit from Lancome, which costs o</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>It leaves your skin glowing, not the next thin...</td>\n",
       "      <td>calming clean, eye con, tone, clean, cleansing...</td>\n",
       "      <td>worth, expensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.tiktok.com/@aliciarev/video/728025...</td>\n",
       "      <td>I also use Lancome's Genifit serum.</td>\n",
       "      <td>Lanc么me</td>\n",
       "      <td>I also use Lancome's Genifit seru</td>\n",
       "      <td>0.8674</td>\n",
       "      <td>positive</td>\n",
       "      <td>I use the Genifit from Lancome, which costs on...</td>\n",
       "      <td>gen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://www.tiktok.com/@aliciarev/video/728025...</td>\n",
       "      <td>It's Eve Pound, the blue one from Drunk Elephant.</td>\n",
       "      <td>Drunk Elephant</td>\n",
       "      <td>blue one from Drunk Elephant.</td>\n",
       "      <td>0.7783</td>\n",
       "      <td>positive</td>\n",
       "      <td>And since I like to use really thick things at...</td>\n",
       "      <td>moist, cream</td>\n",
       "      <td>moist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.tiktok.com/@aliciarev/video/728025...</td>\n",
       "      <td>Then I put on the eyebrow serum, which is Bene...</td>\n",
       "      <td>Benefit Cosmetics</td>\n",
       "      <td>erum, which is Benefit's Huba Brow.</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>positive</td>\n",
       "      <td>They help me so that the nuclear white isn't s...</td>\n",
       "      <td>eyebrow serum, white</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://www.tiktok.com/@aliciarev/video/728025...</td>\n",
       "      <td>And finally, for the lips, the Laneige lip sle...</td>\n",
       "      <td>Laneige</td>\n",
       "      <td>the lips, the Laneige lip sleeping m</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>positive</td>\n",
       "      <td>It costs 71 euros. Well, it works really well....</td>\n",
       "      <td>sleeping mask</td>\n",
       "      <td>chocolate, generous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6769</th>\n",
       "      <td>https://www.tiktok.com/@aleshadows_/video/7454...</td>\n",
       "      <td>The one from the Kerr line from Sephora Collec...</td>\n",
       "      <td>SEPHORA COLLECTION</td>\n",
       "      <td>Kerr line from Sephora Collection.</td>\n",
       "      <td>0.8100</td>\n",
       "      <td>positive</td>\n",
       "      <td>Let's go with the cream contour or bronzer. Of...</td>\n",
       "      <td>bronze</td>\n",
       "      <td>low cost, amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6771</th>\n",
       "      <td>https://www.tiktok.com/@aleshadows_/video/7454...</td>\n",
       "      <td>And again Make Up by Mario, but this time this...</td>\n",
       "      <td>Make Up By Mario</td>\n",
       "      <td>And again Make Up by Mario, but this time</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>positive</td>\n",
       "      <td>The one from the Kerr line from Sephora Collec...</td>\n",
       "      <td>highlight, bal</td>\n",
       "      <td>gorgeous, shade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6776</th>\n",
       "      <td>https://www.tiktok.com/@aleshadows_/video/7454...</td>\n",
       "      <td>A classic every year is the Pingasem from Char...</td>\n",
       "      <td>Charlotte Tilbury</td>\n",
       "      <td>Pingasem from Charlotte Tilbury.</td>\n",
       "      <td>0.8519</td>\n",
       "      <td>positive</td>\n",
       "      <td>I have talked about it a thousand times but it...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>classic, perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6777</th>\n",
       "      <td>https://www.tiktok.com/@aleshadows_/video/7454...</td>\n",
       "      <td>The stick blush from Make Up by Mario in the s...</td>\n",
       "      <td>Make Up By Mario</td>\n",
       "      <td>ick blush from Make Up by Mario in the shade P</td>\n",
       "      <td>0.9052</td>\n",
       "      <td>positive</td>\n",
       "      <td>And last but not least in this video, liquid c...</td>\n",
       "      <td>stick blush, bronze</td>\n",
       "      <td>shade, blush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6779</th>\n",
       "      <td>https://www.tiktok.com/@aleshadows_/video/7454...</td>\n",
       "      <td>And I promise you that Make Up by Mario has no...</td>\n",
       "      <td>Make Up By Mario</td>\n",
       "      <td>omise you that Make Up by Mario has not paid m</td>\n",
       "      <td>0.8151</td>\n",
       "      <td>positive</td>\n",
       "      <td>The stick blush from Make Up by Mario in the s...</td>\n",
       "      <td>powder, blush veil</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1181 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            id_urlvideo  \\\n",
       "15    https://www.tiktok.com/@aliciarev/video/728025...   \n",
       "17    https://www.tiktok.com/@aliciarev/video/728025...   \n",
       "25    https://www.tiktok.com/@aliciarev/video/728025...   \n",
       "30    https://www.tiktok.com/@aliciarev/video/728025...   \n",
       "34    https://www.tiktok.com/@aliciarev/video/728025...   \n",
       "...                                                 ...   \n",
       "6769  https://www.tiktok.com/@aleshadows_/video/7454...   \n",
       "6771  https://www.tiktok.com/@aleshadows_/video/7454...   \n",
       "6776  https://www.tiktok.com/@aleshadows_/video/7454...   \n",
       "6777  https://www.tiktok.com/@aleshadows_/video/7454...   \n",
       "6779  https://www.tiktok.com/@aleshadows_/video/7454...   \n",
       "\n",
       "                                          transcription      brand_detected  \\\n",
       "15    I use the Genifit from Lancome, which costs on...             Lanc么me   \n",
       "17                  I also use Lancome's Genifit serum.             Lanc么me   \n",
       "25    It's Eve Pound, the blue one from Drunk Elephant.      Drunk Elephant   \n",
       "30    Then I put on the eyebrow serum, which is Bene...   Benefit Cosmetics   \n",
       "34    And finally, for the lips, the Laneige lip sle...             Laneige   \n",
       "...                                                 ...                 ...   \n",
       "6769  The one from the Kerr line from Sephora Collec...  SEPHORA COLLECTION   \n",
       "6771  And again Make Up by Mario, but this time this...    Make Up By Mario   \n",
       "6776  A classic every year is the Pingasem from Char...   Charlotte Tilbury   \n",
       "6777  The stick blush from Make Up by Mario in the s...    Make Up By Mario   \n",
       "6779  And I promise you that Make Up by Mario has no...    Make Up By Mario   \n",
       "\n",
       "                                          match_text  sentiment_score  \\\n",
       "15             e Genifit from Lancome, which costs o           0.0000   \n",
       "17                 I also use Lancome's Genifit seru           0.8674   \n",
       "25                     blue one from Drunk Elephant.           0.7783   \n",
       "30               erum, which is Benefit's Huba Brow.           0.7189   \n",
       "34              the lips, the Laneige lip sleeping m           0.8805   \n",
       "...                                              ...              ...   \n",
       "6769              Kerr line from Sephora Collection.           0.8100   \n",
       "6771       And again Make Up by Mario, but this time           0.9725   \n",
       "6776                Pingasem from Charlotte Tilbury.           0.8519   \n",
       "6777  ick blush from Make Up by Mario in the shade P           0.9052   \n",
       "6779  omise you that Make Up by Mario has not paid m           0.8151   \n",
       "\n",
       "     sentiment_label                                       context_text  \\\n",
       "15           neutral  It leaves your skin glowing, not the next thin...   \n",
       "17          positive  I use the Genifit from Lancome, which costs on...   \n",
       "25          positive  And since I like to use really thick things at...   \n",
       "30          positive  They help me so that the nuclear white isn't s...   \n",
       "34          positive  It costs 71 euros. Well, it works really well....   \n",
       "...              ...                                                ...   \n",
       "6769        positive  Let's go with the cream contour or bronzer. Of...   \n",
       "6771        positive  The one from the Kerr line from Sephora Collec...   \n",
       "6776        positive  I have talked about it a thousand times but it...   \n",
       "6777        positive  And last but not least in this video, liquid c...   \n",
       "6779        positive  The stick blush from Make Up by Mario in the s...   \n",
       "\n",
       "                                      detected_products  detected_attributes  \n",
       "15    calming clean, eye con, tone, clean, cleansing...     worth, expensive  \n",
       "17                                                  gen                  NaN  \n",
       "25                                         moist, cream                moist  \n",
       "30                                 eyebrow serum, white                  NaN  \n",
       "34                                        sleeping mask  chocolate, generous  \n",
       "...                                                 ...                  ...  \n",
       "6769                                             bronze    low cost, amazing  \n",
       "6771                                     highlight, bal      gorgeous, shade  \n",
       "6776                                                NaN     classic, perfect  \n",
       "6777                                stick blush, bronze         shade, blush  \n",
       "6779                                 powder, blush veil                  NaN  \n",
       "\n",
       "[1181 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Siguiente paso tengo que coger el archivo este que me devuelve y agrupar las caracteristicas por menciones cercanas. \n",
    "# Poner tambi茅n que si no detecta marca lo deje en blanco no q no ponga no se ha detectado, pq luego el MBA se lia.\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file_path = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\products_attributes\\results\\sentences_transcriptions_processed.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df['brand_detected'] = df['brand_detected'].replace(\"No se detect贸 marca\", \"\")\n",
    "\n",
    "# Agrupar el DataFrame por URL\n",
    "grouped = df.groupby('id_urlvideo')\n",
    "\n",
    "# Nuevo DataFrame para almacenar resultados\n",
    "result_df = df.copy()\n",
    "\n",
    "# Para cada grupo de URL\n",
    "for url, group in grouped:\n",
    "    # Identificar filas con marca\n",
    "    marca_indices = group[group['brand_detected'] != \"\"].index.tolist()\n",
    "    \n",
    "    if not marca_indices:  # Si no hay marcas en este grupo, continuar\n",
    "        continue\n",
    "    \n",
    "    # Para cada fila en el grupo\n",
    "    for idx in group.index:\n",
    "        # Si la fila ya tiene marca, mantener sus productos y atributos\n",
    "        if idx in marca_indices:\n",
    "            continue\n",
    "        \n",
    "        # Encontrar el 铆ndice con marca m谩s cercano\n",
    "        closest_marca_idx = min(marca_indices, key=lambda x: abs(x - idx))\n",
    "        \n",
    "        # Productos detectados en la fila actual\n",
    "        productos = group.loc[idx, 'detected_products']\n",
    "        atributos = group.loc[idx, 'detected_attributes']\n",
    "        \n",
    "        # Si hay productos, a帽adirlos a la fila con marca m谩s cercana\n",
    "        if productos and not pd.isna(productos):\n",
    "            # Obtener productos actuales de la fila con marca\n",
    "            current_products = result_df.loc[closest_marca_idx, 'detected_products']\n",
    "            if pd.isna(current_products) or current_products == \"\":\n",
    "                result_df.loc[closest_marca_idx, 'detected_products'] = productos\n",
    "            else:\n",
    "                # Combinar evitando duplicados\n",
    "                all_products = set(current_products.split(\", \") + productos.split(\", \"))\n",
    "                all_products = {p for p in all_products if p}  # Eliminar cadenas vac铆as\n",
    "                result_df.loc[closest_marca_idx, 'detected_products'] = \", \".join(all_products)\n",
    "        \n",
    "        # Si hay atributos, a帽adirlos a la fila con marca m谩s cercana\n",
    "        if atributos and not pd.isna(atributos):\n",
    "            # Obtener atributos actuales de la fila con marca\n",
    "            current_attrs = result_df.loc[closest_marca_idx, 'detected_attributes']\n",
    "            if pd.isna(current_attrs) or current_attrs == \"\":\n",
    "                result_df.loc[closest_marca_idx, 'detected_attributes'] = atributos\n",
    "            else:\n",
    "                # Combinar evitando duplicados\n",
    "                all_attrs = set(current_attrs.split(\", \") + atributos.split(\", \"))\n",
    "                all_attrs = {a for a in all_attrs if a}  # Eliminar cadenas vac铆as\n",
    "                result_df.loc[closest_marca_idx, 'detected_attributes'] = \", \".join(all_attrs)\n",
    "        \n",
    "        # Limpiar la fila original (sin marca)\n",
    "        result_df.loc[idx, 'detected_products'] = \"\"\n",
    "        result_df.loc[idx, 'detected_attributes'] = \"\"\n",
    "result_df = result_df[result_df['brand_detected'] != \"\"]\n",
    "# Guardar el resultado\n",
    "output_path = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\detection_results\\results_detections.xlsx\"\n",
    "result_df.to_excel(output_path, index=False)\n",
    "print(f\"Archivo guardado en: {output_path}\")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame corregido guardado en: C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\detection_results\\corrected_results_detections.xlsx\n",
      "\n",
      "Ejemplos de correcciones aplicadas:\n",
      "\n",
      "Ejemplo 1:\n",
      "  Productos originales: calming clean, eye con, tone, clean, cleansing oil\n",
      "  Productos corregidos: calming clean, eye con, tone, clean, cleansing oil\n",
      "  Atributos originales: worth, expensive\n",
      "  Atributos corregidos: worth, expensive\n",
      "\n",
      "Ejemplo 2:\n",
      "  Productos originales: gen\n",
      "  Productos corregidos: gen\n",
      "  Atributos originales: nan\n",
      "  Atributos corregidos: nan\n",
      "\n",
      "Ejemplo 3:\n",
      "  Productos originales: moist, cream\n",
      "  Productos corregidos: moist, cream\n",
      "  Atributos originales: moist\n",
      "  Atributos corregidos: moist\n",
      "\n",
      "Ejemplo 4:\n",
      "  Productos originales: eyebrow serum, white\n",
      "  Productos corregidos: eyebrow serum, white\n",
      "  Atributos originales: nan\n",
      "  Atributos corregidos: nan\n",
      "\n",
      "Ejemplo 5:\n",
      "  Productos originales: sleeping mask\n",
      "  Productos corregidos: sleeping mask\n",
      "  Atributos originales: chocolate, generous\n",
      "  Atributos corregidos: chocolate, generous\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import get_close_matches\n",
    "import re\n",
    "\n",
    "def extract_unique_terms(dataframe, column_name):\n",
    "    \"\"\"Funci贸n que saca los valores unicos de columnas con listas separadas por coma.\"\"\"\n",
    "    filtered_df = dataframe[~dataframe[column_name].isna()]\n",
    "    unique_values = set()\n",
    "    for value in filtered_df[column_name]:\n",
    "        if isinstance(value, str) and value.strip():\n",
    "            items = [item.strip() for item in value.split(',')]\n",
    "            unique_values.update([item for item in items if item])\n",
    "    \n",
    "    unique_list = sorted(list(unique_values))\n",
    "    return unique_list\n",
    "\n",
    "unique_products = extract_unique_terms(result_df, 'detected_products')\n",
    "unique_attributes = extract_unique_terms(result_df, 'detected_attributes')\n",
    "\n",
    "# Cargar el archivo con las categor铆as originales\n",
    "categories_path = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\data_bert\\labeled_sentences.xlsx\"\n",
    "categories_df = pd.read_excel(categories_path)\n",
    "product_column = categories_df.columns[2]  # columna de productos_detected\n",
    "attribute_column = categories_df.columns[3] # columna attributes_detected\n",
    "# Categor铆as 煤nicas de los datos etiquetados\n",
    "def extract_product_categories(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # Dividir por comas primero\n",
    "    items = [item.strip() for item in text.split(',')]\n",
    "    products = []\n",
    "    \n",
    "    for item in items:\n",
    "        # Extraer el nombre del producto (ignorando la categor铆a entre par茅ntesis)\n",
    "        match = re.match(r'^(.*?)\\s*\\([^)]*\\)$', item.strip())\n",
    "        if match:\n",
    "            product = match.group(1).strip().lower()\n",
    "            if product:\n",
    "                products.append(product)\n",
    "        else:\n",
    "            # Si no hay par茅ntesis, usar el elemento completo\n",
    "            product = item.strip().lower()\n",
    "            if product:\n",
    "                products.append(product)\n",
    "    \n",
    "    return products\n",
    "\n",
    "def extract_attribute_categories(text):\n",
    "    \"\"\"Extraer Productos y Atributos nicos sin parentesis y sin comas.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # Dividir por comas y limpiar\n",
    "    attributes = [attr.strip().lower() for attr in text.split(',')]\n",
    "    return [attr for attr in attributes if attr]\n",
    "\n",
    "all_product_categories = []\n",
    "for text in categories_df[product_column].dropna():\n",
    "    all_product_categories.extend(extract_product_categories(text))\n",
    "\n",
    "all_attribute_categories = []\n",
    "for text in categories_df[attribute_column].dropna():\n",
    "    all_attribute_categories.extend(extract_attribute_categories(text))\n",
    "\n",
    "unique_product_categories = sorted(set(all_product_categories))\n",
    "unique_attribute_categories = sorted(set(all_attribute_categories))\n",
    "\n",
    "def refined_assign_to_category(term, categories, similar_terms=None, min_term_length=3):\n",
    "    \"\"\"\n",
    "    Funci贸n refinada para asignar t茅rminos a categor铆as, conservando m谩s nombres de productos.\n",
    "    \n",
    "    Args:\n",
    "        term (str): T茅rmino a asignar\n",
    "        categories (list): Lista de categor铆as disponibles\n",
    "        similar_terms (dict, optional): Diccionario de t茅rminos similares o correcciones manuales\n",
    "        min_term_length (int): Longitud m铆nima para considerar un t茅rmino como v谩lido\n",
    "    \n",
    "    Returns:\n",
    "        str: Categor铆a asignada\n",
    "    \"\"\"\n",
    "    # Limpiar el t茅rmino\n",
    "    term_lower = term.lower().strip()\n",
    "    \n",
    "    # Verificar correcciones manuales primero\n",
    "    if similar_terms and term_lower in similar_terms:\n",
    "        return similar_terms[term_lower]\n",
    "    \n",
    "    # Intentar coincidencia exacta\n",
    "    if term_lower in categories:\n",
    "        return term_lower\n",
    "    \n",
    "    # Lista de t茅rminos que definitivamente no son productos/atributos\n",
    "    definite_non_products = ['-', '201', '+', '&', '/', '']\n",
    "    if term_lower in definite_non_products:\n",
    "        return \"other\"\n",
    "    \n",
    "    # Casos especiales con correcciones manuales\n",
    "    if term_lower == \"beauty blend\" or term_lower == \"beauty blender\":\n",
    "        return \"beauty blender\"\n",
    "    \n",
    "    # Para t茅rminos como \"backstage\", \"acid\", etc. que podr铆an ser nombres de productos\n",
    "    # Si tienen una longitud razonable, los mantenemos como est谩n\n",
    "    if len(term_lower) >= min_term_length and not term_lower.isdigit():\n",
    "        # Verificar si es parte de una categor铆a conocida\n",
    "        for category in categories:\n",
    "            pattern = r'\\b' + re.escape(term_lower) + r'\\b'\n",
    "            if re.search(pattern, category):\n",
    "                return category\n",
    "        \n",
    "        # Si no es parte de una categor铆a pero parece un nombre v谩lido, lo dejamos\n",
    "        return term_lower\n",
    "    \n",
    "    # Buscar coincidencias parciales priorizando t茅rminos m谩s largos\n",
    "    sorted_categories = sorted(categories, key=len, reverse=True)\n",
    "    \n",
    "    for category in sorted_categories:\n",
    "        # Comprobar si la categor铆a es una palabra completa dentro del t茅rmino\n",
    "        pattern = r'\\b' + re.escape(category) + r'\\b'\n",
    "        if re.search(pattern, term_lower):\n",
    "            return category\n",
    "    \n",
    "    # Si el t茅rmino es corto pero significativo (como \"bb\" en \"bb cream\")\n",
    "    if term_lower in [\"bb\", \"cc\", \"dd\", \"spf\"] and len(term_lower) == 2:\n",
    "        return term_lower\n",
    "    \n",
    "    # Usar similitud de cadena con umbral m谩s bajo para t茅rminos que parecen nombres\n",
    "    matches = get_close_matches(term_lower, categories, n=1, cutoff=0.7)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    \n",
    "    # Si parece un nombre de producto v谩lido, lo conservamos\n",
    "    if len(term_lower) >= min_term_length and not term_lower.isdigit() and term_lower not in [\"the\", \"and\", \"for\", \"with\"]:\n",
    "        return term_lower\n",
    "    \n",
    "    # Si nada m谩s funciona, devolver \"other\"\n",
    "    return \"other\"\n",
    "\n",
    "# Definir correcciones manuales espec铆ficas\n",
    "manual_corrections_products = {\n",
    "    \"beauty blend\": \"beauty blender\",\n",
    "    \"benefit mascara\": \"mascara\",\n",
    "    \"barrier cream\": \"cream\",\n",
    "    \"backstage\": \"backstage palette\", \n",
    "    \"acid\": \"acid treatment\",\n",
    "    \"bal\": \"balm\",\n",
    "    \"vase\":\"base\",\n",
    "    \"tan\": \"tanned\",\n",
    "    \"ta\": \"tanned\",\n",
    "    \"skin pri\":\"skin primer\",\n",
    "    \"settings break\":\"setting spray\",\n",
    "    \"pillow tail\": \"pillow talk\",\n",
    "    \"pillow\": \"pillow talk\",\n",
    "    \"pen\": \"pencil\",\n",
    "    \"orgasm\":\"pinkgasm\",\n",
    "    \"nose con\":\"contour\",\n",
    "    \"conceal\": \"concealer\",\n",
    "    \"condition\": \"concealer\",\n",
    "    \"con\":\"contour\",\n",
    "    \"nose con\":\"contour\",\n",
    "    \"pen\": \"pencil\",\n",
    "}\n",
    "\n",
    "manual_corrections_attributes = {\n",
    "    \"allergy tested\": \"allergy tested\",\n",
    "    \"amazing\": \"amazing\",\n",
    "    \"good\": \"good\",\n",
    "    \"four shades\":\"shade\",\n",
    "    \"sham\":\"shine\",\n",
    "    \"shadows\":\"shadow\",\n",
    "    \"settings break\":\"setting spray\",\n",
    "    \"pillow tail\": \"pillow talk\",\n",
    "    \"pillow\": \"pillow talk\",\n",
    "    \"orgasm\":\"pinkgasm\",\n",
    "\n",
    "}\n",
    "\n",
    "# Aplicar el mapeo refinado\n",
    "refined_product_mappings = {}\n",
    "for product in unique_products:\n",
    "    refined_product_mappings[product] = refined_assign_to_category(\n",
    "        product, \n",
    "        unique_product_categories, \n",
    "        manual_corrections_products\n",
    "    )\n",
    "\n",
    "refined_attribute_mappings = {}\n",
    "for attribute in unique_attributes:\n",
    "    refined_attribute_mappings[attribute] = refined_assign_to_category(\n",
    "        attribute, \n",
    "        unique_attribute_categories, \n",
    "        manual_corrections_attributes\n",
    "    )\n",
    "# Aplicar los mapeos refinados al DataFrame original\n",
    "def apply_mappings_to_dataframe(df, product_mappings, attribute_mappings):\n",
    "    \"\"\"\n",
    "    Aplica los mapeos de correcci贸n a las columnas de productos y atributos del DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame original\n",
    "        product_mappings: Diccionario de mapeos para productos\n",
    "        attribute_mappings: Diccionario de mapeos para atributos\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con productos y atributos corregidos\n",
    "    \"\"\"\n",
    "    # Crear una copia del DataFrame para no modificar el original\n",
    "    corrected_df = df.copy()\n",
    "    \n",
    "    # Funci贸n para aplicar mapeos a una lista de t茅rminos separados por comas\n",
    "    def apply_mapping_to_list(value, mappings):\n",
    "        if not isinstance(value, str) or not value.strip():\n",
    "            return value\n",
    "        \n",
    "        # Dividir por comas\n",
    "        items = [item.strip() for item in value.split(',')]\n",
    "        \n",
    "        # Aplicar mapeo a cada t茅rmino\n",
    "        corrected_items = []\n",
    "        for item in items:\n",
    "            if item:\n",
    "                # Si el 铆tem est谩 en nuestro mapeo y no es \"other\", usarlo\n",
    "                mapped_item = mappings.get(item.lower(), item)\n",
    "                if mapped_item != \"other\":\n",
    "                    corrected_items.append(mapped_item)\n",
    "                # Si no est谩 en nuestro mapeo, mantener el original\n",
    "                else:\n",
    "                    if len(item) >= 3:  # Mantener solo t茅rminos significativos\n",
    "                        corrected_items.append(item)\n",
    "        \n",
    "        # Unir los t茅rminos corregidos\n",
    "        return \", \".join(corrected_items) if corrected_items else \"\"\n",
    "    \n",
    "    # Aplicar correcciones a la columna de productos\n",
    "    if 'detected_products' in corrected_df.columns:\n",
    "        corrected_df['detected_products'] = corrected_df['detected_products'].apply(\n",
    "            lambda x: apply_mapping_to_list(x, product_mappings)\n",
    "        )\n",
    "    \n",
    "    # Aplicar correcciones a la columna de atributos\n",
    "    if 'detected_attributes' in corrected_df.columns:\n",
    "        corrected_df['detected_attributes'] = corrected_df['detected_attributes'].apply(\n",
    "            lambda x: apply_mapping_to_list(x, attribute_mappings)\n",
    "        )\n",
    "    \n",
    "    return corrected_df\n",
    "\n",
    "# Aplicar las correcciones\n",
    "corrected_result_df = apply_mappings_to_dataframe(result_df, refined_product_mappings, refined_attribute_mappings)\n",
    "\n",
    "# Guardar el DataFrame corregido en un nuevo Excel\n",
    "output_path = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\detection_results\\corrected_results_detections.xlsx\"\n",
    "corrected_result_df.to_excel(output_path, index=False)\n",
    "print(f\"DataFrame corregido guardado en: {output_path}\")\n",
    "\n",
    "# Verificar algunos ejemplos de correcciones\n",
    "print(\"\\nEjemplos de correcciones aplicadas:\")\n",
    "if len(result_df) > 0:\n",
    "    sample_indices = min(5, len(result_df))\n",
    "    for i in range(sample_indices):\n",
    "        original_products = result_df.iloc[i]['detected_products'] if 'detected_products' in result_df.columns else ''\n",
    "        corrected_products = corrected_result_df.iloc[i]['detected_products'] if 'detected_products' in corrected_result_df.columns else ''\n",
    "        \n",
    "        original_attributes = result_df.iloc[i]['detected_attributes'] if 'detected_attributes' in result_df.columns else ''\n",
    "        corrected_attributes = corrected_result_df.iloc[i]['detected_attributes'] if 'detected_attributes' in corrected_result_df.columns else ''\n",
    "        \n",
    "        print(f\"\\nEjemplo {i+1}:\")\n",
    "        print(f\"  Productos originales: {original_products}\")\n",
    "        print(f\"  Productos corregidos: {corrected_products}\")\n",
    "        print(f\"  Atributos originales: {original_attributes}\")\n",
    "        print(f\"  Atributos corregidos: {corrected_attributes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo CSV: C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\data\\clean_data\\sephora_website_cleaned.csv\n",
      "Procesando datos y extrayendo t茅rminos significativos por marca...\n",
      "Archivo guardado en: C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Kmeans\\data for kmeans\\brand_meaningful_terms_forkmeans.xlsx\n",
      "Total de marcas procesadas: 147\n",
      "\n",
      "Ejemplos de las primeras marcas procesadas:\n",
      "\n",
      "Marca: SEPHORA COLLECTION\n",
      "Promedio de precio: $18.91\n",
      "N煤mero de productos: 142\n",
      "T茅rminos significativos: vitamin e, caffeine, natural, concealer, limited-edition, brightening, mascara, primer, professional, tea tree, cream, full-coverage, hypoallergenic, powder, glossy, lip gloss, ceramides, niacinamide, plumping, metallic, alcohol-free, contour, lipstick, balm, highlighter, hyaluronic acid, vitamin c, bronzer, oil, lip balm\n",
      "\n",
      "Marca: Anastasia Beverly Hills\n",
      "Promedio de precio: $25.18\n",
      "N煤mero de productos: 83\n",
      "T茅rminos significativos: vitamin e, peptides, premium, natural, concealer, dermatologist-tested, mascara, primer, professional, cream, full-coverage, powder, shimmery, lip gloss, niacinamide, plumping, dewy, metallic, contour, lipstick, balm, highlighter, hyaluronic acid, bronzer, oil, eyeliner, foundation, serum, eyeshadow, hydrating\n",
      "\n",
      "Marca: tarte\n",
      "Promedio de precio: $28.54\n",
      "N煤mero de productos: 82\n",
      "T茅rminos significativos: vitamin e, peptides, natural, concealer, limited-edition, brightening, dermatologist-tested, mascara, primer, cream, full-coverage, hypoallergenic, powder, glossy, lip gloss, spf, antioxidants, niacinamide, plumping, dewy, metallic, contour, lipstick, balm, highlighter, hyaluronic acid, vitamin c, bronzer, oil, lip balm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Configurar rutas\n",
    "input_file = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\data\\clean_data\\sephora_website_cleaned.csv\"\n",
    "output_dir = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Kmeans\\data for kmeans\"\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Leer el archivo CSV de Sephora\n",
    "print(f\"Leyendo archivo CSV: {input_file}\")\n",
    "sephora_df = pd.read_csv(input_file)\n",
    "\n",
    "# Definir t茅rminos espec铆ficos que caracterizan productos cosm茅ticos (convertido a conjunto)\n",
    "cosmetic_specific_terms = {\n",
    "    # Tipo de producto\n",
    "    'foundation', 'concealer', 'powder', 'blush', 'bronzer', 'highlighter', \n",
    "    'eyeshadow', 'eyeliner', 'mascara', 'lipstick', 'lip gloss', 'lip balm', \n",
    "    'primer', 'setting spray', 'face mask', 'moisturizer', 'serum', 'toner', \n",
    "    'cleanser', 'eye cream', 'sunscreen', 'bb cream', 'cc cream', 'brow pencil',\n",
    "    'palette', 'contour', 'illuminator', 'sponge', 'beauty blender',\n",
    "    \n",
    "    # Texturas y acabados\n",
    "    'matte', 'dewy', 'glowy', 'radiant', 'luminous', 'shimmery', 'glossy', 'satin',\n",
    "    'metallic', 'glitter', 'creamy', 'whipped', 'gel', 'liquid', 'cream',\n",
    "    'mousse', 'balm', 'oil', 'stick', 'cushion', 'lightweight', 'full-coverage',\n",
    "    \n",
    "    # Ingredientes clave\n",
    "    'retinol', 'vitamin c', 'hyaluronic acid', 'peptides', 'niacinamide', 'aha',\n",
    "    'bha', 'salicylic acid', 'glycolic acid', 'lactic acid', 'ceramides', 'spf',\n",
    "    'antioxidants', 'collagen', 'aloe', 'shea butter', 'tea tree', 'squalane',\n",
    "    'bakuchiol', 'vitamin e', 'zinc', 'caffeine', 'centella', 'propolis',\n",
    "    \n",
    "    # Caracter铆sticas y beneficios\n",
    "    'vegan', 'cruelty-free', 'clean', 'organic', 'natural', 'dermatologist-tested',\n",
    "    'hypoallergenic', 'non-comedogenic', 'paraben-free', 'sulfate-free', 'silicone-free',\n",
    "    'fragrance-free', 'alcohol-free', 'gluten-free', 'oil-free', 'waterproof',\n",
    "    'longwear', 'transfer-proof', 'buildable', 'hydrating', 'moisturizing', 'brightening',\n",
    "    'anti-aging', 'firming', 'plumping', 'exfoliating', 'soothing', 'calming',\n",
    "    \n",
    "    # Estilo y mercado\n",
    "    'luxury', 'premium', 'affordable', 'professional', 'korean', 'japanese', 'french',\n",
    "    'k-beauty', 'j-beauty', 'inclusive', 'sustainable', 'refillable', 'travel-sized',\n",
    "    'limited-edition', 'customizable', 'unisex', 'cruelty free'\n",
    "}\n",
    "\n",
    "# Frases a excluir del formato de descripci贸n de Sephora (convertido a conjunto)\n",
    "sephora_format_phrases = {\n",
    "    'what it is', 'what else you need to know', 'what else you need', 'what else', \n",
    "    'you need to know', 'you need to', 'need to know', 'need to', 'to know',\n",
    "    'it is', 'it is a', 'is a', 'this is', 'this is a', 'that is', 'that is a',\n",
    "    'else you need', 'else you', 'with a', 'without a', 'of the', 'in the', 'on the',\n",
    "    'for the', 'at the', 'by the', 'from the', 'to the', 'with the', 'and the',\n",
    "    'highlighted ingredients', 'ingredient callouts', 'formulation type', 'coverage',\n",
    "    'finish', 'benefits', 'formulation', 'why we like it', 'how to use', 'what you get',\n",
    "    'key ingredients', 'key benefits', 'suggested usage', 'free of', 'made with',\n",
    "    'contains', 'includes', 'helps', 'helps to', 'designed to', 'works to',\n",
    "    'makes', 'gives', 'provides', 'delivers', 'offers', 'features', 'perfect for',\n",
    "    'ideal for', 'great for', 'suitable for', 'specially formulated',\n",
    "    'clinically proven', 'dermatologist tested', 'ophthalmologist tested'\n",
    "}\n",
    "\n",
    "# Palabras gen茅ricas a excluir (convertido a conjunto)\n",
    "generic_words = {\n",
    "    'it', 'is', 'it is', 'this', 'this is', 'that', 'that is', 'the', 'and', 'are', 'these', \n",
    "    'those', 'with', 'without', 'synthetic', 'else you need', 'else you', 'you need to', \n",
    "    'need to know', 'need to', 'to know', 'what it is', 'is a', 'what else you', 'what else', \n",
    "    'help', 'contains', 'it is a', 'from', 'into', 'during', 'before', 'after', 'above', \n",
    "    'below', 'between', 'among', 'throughout', 'through', 'over', 'under', 'within', 'along', \n",
    "    'across', 'around', 'about', 'against', 'beyond', 'near', 'same', 'different', 'various',\n",
    "    'several', 'few', 'many', 'much', 'some', 'any', 'all', 'every', 'each', 'either',\n",
    "    'neither', 'both', 'such', 'like', 'unlike', 'similar', 'other', 'another',\n",
    "    'than', 'then', 'now', 'later', 'earlier', 'soon', 'already', 'still', 'yet',\n",
    "    'once', 'twice', 'again', 'ever', 'never', 'always', 'often', 'seldom', 'usually',\n",
    "    'generally', 'sometimes', 'rarely', 'frequently', 'occasionally', 'normally',\n",
    "    'commonly', 'naturally', 'especially', 'particularly', 'specifically', 'exactly',\n",
    "    'precisely', 'approximately', 'roughly', 'nearly', 'almost', 'virtually', 'practically',\n",
    "    'essentially', 'basically', 'fundamentally', 'primarily', 'mainly', 'mostly',\n",
    "    'largely', 'chiefly', 'principally', 'predominantly', 'significantly',\n",
    "    'notably', 'markedly', 'considerably', 'substantially', 'extensively', 'thoroughly',\n",
    "    'completely', 'entirely', 'totally', 'wholly', 'fully', 'partly', 'partially',\n",
    "    'somewhat', 'slightly', 'relatively', 'comparatively', 'rather', 'quite', 'extremely',\n",
    "    'very', 'too', 'enough', 'sufficiently', 'adequately', 'properly', 'correctly',\n",
    "    'accordingly', 'consequently', 'therefore', 'thus', 'hence', 'so', 'otherwise',\n",
    "    'nonetheless', 'nevertheless', 'however', 'although', 'though', 'even', 'just',\n",
    "    'only', 'merely', 'simply', 'really', 'actually', 'fact', 'indeed', 'certainly',\n",
    "    'definitely', 'absolutely', 'undoubtedly', 'surely', 'clearly', 'obviously',\n",
    "    'apparently', 'seemingly', 'possibly', 'perhaps', 'maybe', 'probably',\n",
    "    'likely', 'unlikely', 'doubtfully', 'supposedly', 'allegedly', 'reportedly',\n",
    "    'furthermore', 'moreover', 'additionally', 'also', 'besides',\n",
    "    'likewise', 'similarly', 'instead', 'alternatively', 'conversely',\n",
    "    'whereas', 'while', 'because', 'since', 'due', 'owing', 'result',\n",
    "    'subsequently', 'next', 'last', 'finally', 'ultimately', 'eventually', \n",
    "    'steadily', 'continuously', 'constantly', 'persistently', 'consistently', 'regularly',\n",
    "    'periodically', 'intermittently', 'sporadically', 'randomly', 'suddenly', 'abruptly',\n",
    "    'immediately', 'instantly', 'quickly', 'rapidly', 'swiftly', 'promptly', 'hastily',\n",
    "    'speedily', 'slowly', 'deliberately', 'purposefully', 'intentionally',\n",
    "    'knowingly', 'unwittingly', 'accidentally', 'mistakenly', 'erroneously',\n",
    "    'incorrectly', 'rightly', 'appropriately', 'suitably',\n",
    "    'satisfactorily', 'effectively', 'efficiently', 'successfully', 'poorly', 'badly',\n",
    "    'terribly', 'horribly', 'awfully', 'dreadfully', 'wonderfully', 'beautifully',\n",
    "    'nicely', 'pleasantly', 'delightfully', 'fortunately', 'unfortunately', 'happily',\n",
    "    'sadly', 'regrettably', 'hopefully', 'presumably',\n",
    "    'evidently', 'unquestionably', 'indisputably', 'indubitably', 'no', 'not', 'none', \n",
    "    'nobody', 'nothing', 'nowhere', 'nor', 'one', 'everything', 'everyone', 'everybody',\n",
    "    'everywhere', 'both', 'someone', 'somebody', 'something', 'somewhere', 'sometime', \n",
    "    'anyone', 'anybody', 'anything', 'anywhere', 'anytime',\n",
    "    \n",
    "    # Palabras espec铆ficas del dominio que queremos excluir\n",
    "    'is', 'application', 'collection', 'bristles', 'know', 'skin', 'brushes', \"what\",\"liquid\", 'lips', 'brow', 'finish', 'brows', 'formula', 'color',\n",
    "    'reflects', 'shades', 'parabens',\n",
    "    'brand', 'product', 'products', 'item', 'items', 'use', 'using', 'used',\n",
    "    'look', 'looks', 'looking', 'apply', 'applied', 'applying', 'wear', 'wearing',\n",
    "    'choose', 'time', 'times', 'day', 'days', 'week', 'weeks', 'month', 'months',\n",
    "    'year', 'years', 'size', 'small', 'medium', 'large', 'mini', 'full',\n",
    "    'new', 'old', 'fresh', 'set', 'sets', 'kit', 'kits', 'bundle', 'feature',\n",
    "    'features', 'benefit', 'benefits', 'include', 'includes', 'included',\n",
    "    'contain', 'contains', 'contained', 'create', 'creates', 'created',\n",
    "    'provide', 'provides', 'provided', 'offer', 'offers', 'offered',\n",
    "    'exclusive', 'recommend', 'recommended', 'favorite', 'favorites',\n",
    "    'best', 'better', 'good', 'great', 'excellent', 'amazing', 'perfect',\n",
    "    'love', 'loved', 'loves', 'loving', 'like', 'liked', 'likes', 'liking',\n",
    "    'works', 'worked', 'working', 'want', 'wanted', 'wanting', 'wants',\n",
    "    'designed', 'designed for', 'specially', 'special', 'specific', 'specifically',\n",
    "    'designed to', 'made to', 'formulated', 'formulated to', 'specially formulated'\n",
    "}\n",
    "\n",
    "def extract_meaningful_terms(text, target_terms=cosmetic_specific_terms, \n",
    "                             excluded_phrases=sephora_format_phrases,\n",
    "                             generic_stopwords=generic_words):\n",
    "    \"\"\"\n",
    "    Extrae t茅rminos significativos y representativos espec铆ficos del sector cosm茅tico,\n",
    "    evitando frases del formato de descripci贸n de Sephora y palabras gen茅ricas.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Convertir a min煤sculas y limpieza inicial\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s-]', ' ', text)  # Reemplazar puntuaci贸n conservando guiones\n",
    "    text = re.sub(r'\\s+', ' ', text)       # Normalizar espacios\n",
    "    \n",
    "    # Preprocesamiento: eliminar frases del formato de descripciones de Sephora\n",
    "    processed_text = ' ' + text + ' '  # A帽adir espacios para detectar palabras completas\n",
    "    for phrase in excluded_phrases:\n",
    "        processed_text = processed_text.replace(' ' + phrase + ' ', ' ')\n",
    "    \n",
    "    # Volver a normalizar espacios despu茅s de eliminar frases\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
    "    \n",
    "    # Buscar t茅rminos espec铆ficos de cosm茅tica en el texto procesado\n",
    "    found_terms = []\n",
    "    \n",
    "    # Primero buscar t茅rminos espec铆ficos de cosm茅tica (prioridad)\n",
    "    for term in target_terms:\n",
    "        # Verificar si el t茅rmino aparece como palabra completa\n",
    "        pattern = r'\\b' + re.escape(term) + r'\\b'\n",
    "        if re.search(pattern, processed_text):\n",
    "            # Verificar que ninguna palabra del t茅rmino est茅 en la lista de stopwords\n",
    "            term_words = term.split()\n",
    "            if not any(word in generic_stopwords for word in term_words):\n",
    "                found_terms.append(term)\n",
    "    \n",
    "    # Buscar palabras individuales que podr铆an ser relevantes\n",
    "    individual_words = set(re.findall(r'\\b[a-z]{3,}\\b', processed_text))\n",
    "    \n",
    "    # Filtrar palabras individuales contra stopwords\n",
    "    for word in individual_words:\n",
    "        if (word not in generic_stopwords and \n",
    "            word not in found_terms and \n",
    "            not any(word in term.split() for term in found_terms)):\n",
    "            found_terms.append(word)\n",
    "    \n",
    "    # Contar frecuencias de los t茅rminos encontrados\n",
    "    term_counts = Counter(found_terms)\n",
    "    \n",
    "    # Filtro final: verificar una vez m谩s que ning煤n t茅rmino contenga palabras en generic_stopwords\n",
    "    filtered_terms = Counter()\n",
    "    for term, count in term_counts.items():\n",
    "        term_words = term.split()\n",
    "        if not any(word in generic_stopwords for word in term_words):\n",
    "            filtered_terms[term] = count\n",
    "    \n",
    "    return filtered_terms\n",
    "\n",
    "# Preparar el dataframe\n",
    "print(\"Procesando datos y extrayendo t茅rminos significativos por marca...\")\n",
    "\n",
    "# Combinar columnas de descripci贸n\n",
    "description_columns = [col for col in ['description', 'What it is', 'Finish', 'What Else You Need to Know'] \n",
    "                     if col in sephora_df.columns]\n",
    "\n",
    "# Crear una columna combinada de descripciones\n",
    "sephora_df['combined_description'] = sephora_df[description_columns].apply(\n",
    "    lambda row: ' '.join([str(cell) for cell in row if not pd.isna(cell)]), axis=1\n",
    ")\n",
    "\n",
    "# Agrupar por marca\n",
    "brand_groups = sephora_df.groupby('brand')\n",
    "\n",
    "# Lista para almacenar los resultados\n",
    "brand_data = []\n",
    "\n",
    "# Procesar cada marca\n",
    "for brand, group in brand_groups:\n",
    "    # Combinar todas las descripciones de esta marca\n",
    "    all_descriptions = ' '.join(group['combined_description'].tolist())\n",
    "    \n",
    "    # Extraer t茅rminos significativos\n",
    "    term_counts = extract_meaningful_terms(all_descriptions)\n",
    "    \n",
    "    # Obtener los 30 t茅rminos m谩s frecuentes\n",
    "    top_terms = [term for term, count in term_counts.most_common(30)]\n",
    "    \n",
    "    # Calcular el precio promedio de los productos de esta marca\n",
    "    avg_price = group['current_price'].astype(float).mean() if 'current_price' in group.columns else np.nan\n",
    "    \n",
    "    # Contar n煤mero de productos de esta marca\n",
    "    num_products = len(group)\n",
    "    \n",
    "    # Guardar datos de esta marca\n",
    "    brand_data.append({\n",
    "        'brand': brand,\n",
    "        'representative_terms': ', '.join(top_terms),\n",
    "        'avg_price': avg_price,\n",
    "        'num_products': num_products\n",
    "    })\n",
    "\n",
    "# Crear dataframe con la informaci贸n por marca\n",
    "brand_df = pd.DataFrame(brand_data)\n",
    "\n",
    "# Ordenar por n煤mero de productos (marcas con m谩s productos primero)\n",
    "brand_df = brand_df.sort_values('num_products', ascending=False)\n",
    "\n",
    "# Guardar en Excel\n",
    "output_file = os.path.join(output_dir, 'brand_meaningful_terms_forkmeans.xlsx')\n",
    "brand_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: {output_file}\")\n",
    "print(f\"Total de marcas procesadas: {len(brand_df)}\")\n",
    "\n",
    "# Mostrar algunos ejemplos\n",
    "print(\"\\nEjemplos de las primeras marcas procesadas:\")\n",
    "for i, row in brand_df.head(3).iterrows():\n",
    "    print(f\"\\nMarca: {row['brand']}\")\n",
    "    print(f\"Promedio de precio: ${row['avg_price']:.2f}\")\n",
    "    print(f\"N煤mero de productos: {row['num_products']}\")\n",
    "    print(f\"T茅rminos significativos: {row['representative_terms']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
