{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: torch in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: seqeval in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (4.66.6)\n",
      "Requirement already satisfied: torch in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (2.4.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers[torch]) (1.5.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch->transformers[torch]) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sandr\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Mobile bert\n",
    "!pip install transformers datasets torch pandas numpy seqeval\n",
    "!pip install tensorflow\n",
    "! pip install tf-keras\n",
    "! pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileBERT Fine-tuning para Detección de Productos Cosméticos y Atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sandr\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DEPENDENCIAS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from seqeval.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuración\n",
    "RUTA_DATOS = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\data_bert\\labeled_sentences.xlsx\"\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\products_attributes\"\n",
    "BATCH_SIZE = 9\n",
    "LEARNING_RATE = 3e-5\n",
    "NUM_EPOCHS = 9\n",
    "MAX_LENGTH = 128\n",
    "WARMUP_RATIO = 0.1   \n",
    "SEED = 6\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.2\n",
    "VALIDATION_SPLIT_RATIO = 0.5  # Del conjunto de test, la mitad será para validación\n",
    "\n",
    "# Configuración de etiquetas\n",
    "# Usaremos esquema BIO (Beginning, Inside, Outside) para entidades\n",
    "# B-PRODUCT: Inicio de producto\n",
    "# I-PRODUCT: Continuación de producto\n",
    "# B-ATTRIBUTE: Inicio de atributo\n",
    "# I-ATTRIBUTE: Continuación de atributo\n",
    "# O: No es parte de una entidad\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos etiquetados...\n",
      "Datos cargados correctamente. Total de filas: 489\n",
      "Columnas disponibles: id_urlvideo, transcription, products_detected, attributes_detected\n",
      "                                         id_urlvideo  \\\n",
      "0  https://www.tiktok.com/@scurtoworld/video/6868...   \n",
      "1  https://www.tiktok.com/@missprettygirl/video/7...   \n",
      "2  https://www.tiktok.com/@mariane.liebhaber/vide...   \n",
      "3  https://www.tiktok.com/@fake.it.tilu.make.it/v...   \n",
      "4  https://www.tiktok.com/@hunterdestin/video/745...   \n",
      "\n",
      "                                       transcription  \\\n",
      "0  No shadow for me today, but we're gonna have t...   \n",
      "1  Looks like a brown liner and then like a pink ...   \n",
      "2        Next thing are these touch land power mist.   \n",
      "3  And I upgraded to the glass bottles, not the b...   \n",
      "4      So we'll kind of like mix these two together.   \n",
      "\n",
      "                                 products_detected      attributes_detected  \n",
      "0                                eyeliner (makeup)                      NaN  \n",
      "1                   liner (makeup), gloss (makeup)              brown, pink  \n",
      "2                            power mist (skincare)                      NaN  \n",
      "3  glass bottles (packaging), body spray(skincare)  upgrade from body spray  \n",
      "4                                              NaN                  mixable  \n",
      "\n",
      "Preprocesando datos...\n"
     ]
    }
   ],
   "source": [
    "# 1. Carga de datos etiquetados\n",
    "print(\"Cargando datos etiquetados...\")\n",
    "try:\n",
    "    df = pd.read_excel(RUTA_DATOS)\n",
    "    print(f\"Datos cargados correctamente. Total de filas: {len(df)}\")\n",
    "    print(f\"Columnas disponibles: {', '.join(df.columns)}\")\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar los datos: {str(e)}\")\n",
    "\n",
    "print(\"\\nPreprocesando datos...\")\n",
    "\n",
    "# Función para dividir texto en tokens a nivel de palabra\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Tokeniza un texto en palabras.\"\"\"\n",
    "    # Manejar NaN o valores vacíos\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return []\n",
    "    # Tokenización simple por espacios y puntuación\n",
    "    tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
    "    return tokens\n",
    "\n",
    "# Función para convertir etiquetas de productos y atributos a formato BIO\n",
    "def create_bio_tags(text, products, attributes):\n",
    "    \"\"\"\n",
    "    Crea etiquetas BIO para un texto dado productos y atributos.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto original\n",
    "        products (str): Lista de productos separados por comas\n",
    "        attributes (str): Lista de atributos separados por comas\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de etiquetas BIO alineadas con tokens del texto\n",
    "    \"\"\"\n",
    "    tokens = simple_tokenize(text)\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    # Inicializar todas las etiquetas como \"O\" (Outside)\n",
    "    bio_tags = [\"O\"] * len(tokens)\n",
    "    \n",
    "    # Procesar productos\n",
    "    if not pd.isna(products) and products.strip() != \"\":\n",
    "        product_list = [p.strip().lower() for p in products.split(',')]\n",
    "        for product in product_list:\n",
    "            # Extraer el nombre del producto sin la categoría entre paréntesis\n",
    "            match = re.match(r\"(.*?)(\\s*\\(.*\\))?$\", product)\n",
    "            if match:\n",
    "                product_name = match.group(1).strip().lower()\n",
    "                product_tokens = simple_tokenize(product_name)\n",
    "                \n",
    "                if product_tokens:\n",
    "                    # Buscar el producto en el texto tokenizado\n",
    "                    for i in range(len(tokens) - len(product_tokens) + 1):\n",
    "                        if [t.lower() for t in tokens[i:i+len(product_tokens)]] == [t.lower() for t in product_tokens]:\n",
    "                            # Marcar el primer token como B-PRODUCT\n",
    "                            bio_tags[i] = \"B-PRODUCT\"\n",
    "                            # Marcar los tokens restantes como I-PRODUCT\n",
    "                            for j in range(1, len(product_tokens)):\n",
    "                                bio_tags[i + j] = \"I-PRODUCT\"\n",
    "    \n",
    "    # Procesar atributos\n",
    "    if not pd.isna(attributes) and attributes.strip() != \"\":\n",
    "        attribute_list = [a.strip().lower() for a in attributes.split(',')]\n",
    "        for attribute in attribute_list:\n",
    "            attribute_tokens = simple_tokenize(attribute)\n",
    "            \n",
    "            if attribute_tokens:\n",
    "                # Buscar el atributo en el texto tokenizado\n",
    "                for i in range(len(tokens) - len(attribute_tokens) + 1):\n",
    "                    if [t.lower() for t in tokens[i:i+len(attribute_tokens)]] == [t.lower() for t in attribute_tokens]:\n",
    "                        # Marcar el primer token como B-ATTRIBUTE\n",
    "                        bio_tags[i] = \"B-ATTRIBUTE\"\n",
    "                        # Marcar los tokens restantes como I-ATTRIBUTE\n",
    "                        for j in range(1, len(attribute_tokens)):\n",
    "                            bio_tags[i + j] = \"I-ATTRIBUTE\"\n",
    "    \n",
    "    return list(zip(tokens, bio_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223bd9c3cb00495a9c28cdeb5d902624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creando etiquetas BIO:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesamiento completado. Datos tokenizados: 489. Filas omitidas: 0\n"
     ]
    }
   ],
   "source": [
    "# Aplicar el procesamiento a nuestros datos\n",
    "tokenized_data = []\n",
    "skipped_rows = 0\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Creando etiquetas BIO\"):\n",
    "    try:\n",
    "        text = row['transcription']\n",
    "        products = row['products_detected'] if 'products_detected' in row else \"\"\n",
    "        attributes = row['attributes_detected'] if 'attributes_detected' in row else \"\"\n",
    "        \n",
    "        if pd.isna(text) or text.strip() == \"\":\n",
    "            skipped_rows += 1\n",
    "            continue\n",
    "            \n",
    "        token_tag_pairs = create_bio_tags(text, products, attributes)\n",
    "        \n",
    "        if token_tag_pairs:\n",
    "            tokenized_data.append({\n",
    "                'id': i,\n",
    "                'tokens': [pair[0] for pair in token_tag_pairs],\n",
    "                'bio_tags': [pair[1] for pair in token_tag_pairs],\n",
    "                'text': text,\n",
    "                'products': products if not pd.isna(products) else \"\",\n",
    "                'attributes': attributes if not pd.isna(attributes) else \"\"\n",
    "            })\n",
    "        else:\n",
    "            skipped_rows += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la fila {i}: {str(e)}\")\n",
    "        skipped_rows += 1\n",
    "\n",
    "print(f\"Procesamiento completado. Datos tokenizados: {len(tokenized_data)}. Filas omitidas: {skipped_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribución de etiquetas:\n",
      "I-ATTRIBUTE: 202 (2.63%)\n",
      "I-PRODUCT: 114 (1.48%)\n",
      "B-ATTRIBUTE: 380 (4.94%)\n",
      "O: 6638 (86.26%)\n",
      "B-PRODUCT: 361 (4.69%)\n"
     ]
    }
   ],
   "source": [
    "# Revisar estadísticas de etiquetas\n",
    "all_tags = [tag for item in tokenized_data for tag in item['bio_tags']]\n",
    "tag_counts = {tag: all_tags.count(tag) for tag in set(all_tags)}\n",
    "print(\"\\nDistribución de etiquetas:\")\n",
    "for tag, count in tag_counts.items():\n",
    "    print(f\"{tag}: {count} ({count/len(all_tags)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución de Etiquetas:\n",
    "- **O (Outside)**: 94.93%  \n",
    "- **B-PRODUCT (Inicio de productos)**: 1.88%  \n",
    "- **I-PRODUCT (Continuación de productos)**: 0.38%  \n",
    "- **B-ATTRIBUTE (Inicio de atributos)**: 2.27%  \n",
    "- **I-ATTRIBUTE (Continuación de atributos)**: 0.55%  \n",
    "\n",
    "Este tipo de desequilibrio es bastante común en tareas de Named Entity Recognition (NER) porque la mayoría de las palabras en un texto no suelen ser entidades nombradas. Sin embargo, podría afectar el rendimiento del modelo, ya que hay pocas muestras de las clases minoritarias.  \n",
    "\n",
    "#### Para Abordar el Desequilibrio:\n",
    "\n",
    "- Sobremuestreo de Clases Minoritarias: Etiquetar más ejemplos que contengan productos y atributos para aumentar su representación en los datos de entrenamiento.  \n",
    "-  Ajustar los Pesos de las Clases: Se puede modificar el código para dar más peso a las clases minoritarias durante el entrenamiento:  \n",
    "```python\n",
    "# En la definición de TrainingArguments\n",
    "class_weights = torch.tensor([1.0, 20.0, 30.0, 20.0, 30.0])  # Ajustar estos valores según necesidades\n",
    "loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "```\n",
    "- Regularización Más Fuerte: Para evitar que el modelo simplemente prediga \"O\" para todo, se puede aumentar la penalización por sobreajuste:  \n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    # Otros parámetros...\n",
    "    weight_decay=0.1  # Aumentar el valor original (por ejemplo, de 0.01 a 0.1)\n",
    ")\n",
    "```\n",
    "- Técnicas de Data Augmentation: Generar más ejemplos sintéticos para aumentar la representación de productos y atributos en el conjunto de entrenamiento.  \n",
    "- Ajustar el Número de Épocas: Considerar aumentar el número de épocas de entrenamiento para que el modelo tenga más oportunidades de aprender los patrones menos frecuentes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dividiendo datos en conjuntos de entrenamiento, validación y prueba...\n",
      "Conjunto de entrenamiento: 391 ejemplos\n",
      "Conjunto de validación: 49 ejemplos\n",
      "Conjunto de prueba: 49 ejemplos\n"
     ]
    }
   ],
   "source": [
    "# 3. Dividir en conjuntos de entrenamiento, validación y prueba\n",
    "print(\"\\nDividiendo datos en conjuntos de entrenamiento, validación y prueba...\")\n",
    "\n",
    "train_data, test_data = train_test_split(\n",
    "    tokenized_data, \n",
    "    test_size=TRAIN_TEST_SPLIT_RATIO, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "val_data, test_data = train_test_split(\n",
    "    test_data, \n",
    "    test_size=VALIDATION_SPLIT_RATIO, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Conjunto de entrenamiento: {len(train_data)} ejemplos\")\n",
    "print(f\"Conjunto de validación: {len(val_data)} ejemplos\")\n",
    "print(f\"Conjunto de prueba: {len(test_data)} ejemplos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparando datos para MobileBERT...\n",
      "Mapeo de etiquetas: {'B-ATTRIBUTE': 0, 'B-PRODUCT': 1, 'I-ATTRIBUTE': 2, 'I-PRODUCT': 3, 'O': 4}\n"
     ]
    }
   ],
   "source": [
    "# 4. Preparación para MobileBERT\n",
    "print(\"\\nPreparando datos para MobileBERT...\")\n",
    "\n",
    "# Cargar tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Definir mapeo de etiquetas a IDs\n",
    "unique_tags = sorted(list(set(all_tags)))\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "\n",
    "# Guardar el mapeo para uso futuro\n",
    "with open(os.path.join(OUTPUT_DIR, 'tag_mapping.json'), 'w') as f:\n",
    "    json.dump({'tag2id': tag2id, 'id2tag': id2tag}, f)\n",
    "\n",
    "print(f\"Mapeo de etiquetas: {tag2id}\")\n",
    "\n",
    "# Clase para tokenizar y preparar los datos para MobileBERT\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        tokens = item['tokens']\n",
    "        bio_tags = item['bio_tags']\n",
    "        \n",
    "        # Tokenizar palabras usando el tokenizador de MobileBERT\n",
    "        word_ids = []\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        labels = []\n",
    "        \n",
    "        # Añadir token [CLS] al inicio\n",
    "        input_ids.append(tokenizer.cls_token_id)\n",
    "        attention_mask.append(1)\n",
    "        word_ids.append(None)\n",
    "        labels.append(-100)  # -100 es ignorado en la función de pérdida\n",
    "        \n",
    "        # Tokenizar cada palabra y alinear con etiquetas BIO\n",
    "        for word_idx, (word, tag) in enumerate(zip(tokens, bio_tags)):\n",
    "            # Tokenizar la palabra (podría dar múltiples tokens)\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                # Si la palabra está fuera del vocabulario o es ignorada, usamos el token desconocido\n",
    "                word_tokens = [tokenizer.unk_token]\n",
    "                \n",
    "            # Añadir IDs de tokens\n",
    "            for i, _ in enumerate(word_tokens):\n",
    "                input_ids.append(tokenizer.convert_tokens_to_ids(word_tokens[i]))\n",
    "                attention_mask.append(1)\n",
    "                word_ids.append(word_idx)\n",
    "                \n",
    "                # Solo el primer subtoken mantiene la etiqueta, el resto se ignora\n",
    "                if i == 0:\n",
    "                    labels.append(tag2id[tag])\n",
    "                else:\n",
    "                    labels.append(-100)\n",
    "        \n",
    "        # Añadir token [SEP] al final\n",
    "        input_ids.append(tokenizer.sep_token_id)\n",
    "        attention_mask.append(1)\n",
    "        word_ids.append(None)\n",
    "        labels.append(-100)\n",
    "        \n",
    "        # Rellenar o truncar a max_length\n",
    "        padding_length = self.max_length - len(input_ids)\n",
    "        \n",
    "        if padding_length > 0:\n",
    "            # Rellenar\n",
    "            input_ids.extend([tokenizer.pad_token_id] * padding_length)\n",
    "            attention_mask.extend([0] * padding_length)\n",
    "            labels.extend([-100] * padding_length)\n",
    "            word_ids.extend([None] * padding_length)\n",
    "        elif padding_length < 0:\n",
    "            # Truncar\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "            attention_mask = attention_mask[:self.max_length]\n",
    "            labels = labels[:self.max_length]\n",
    "            word_ids = word_ids[:self.max_length]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'word_ids': word_ids  # Esto es para debugging, no se usa en entrenamiento\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurando y entrenando el modelo MobileBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sandr\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\sandr\\AppData\\Local\\Temp\\ipykernel_12088\\2872197540.py:127: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Crear conjuntos de datos\n",
    "train_dataset = NERDataset(train_data, tokenizer, MAX_LENGTH)\n",
    "val_dataset = NERDataset(val_data, tokenizer, MAX_LENGTH)\n",
    "test_dataset = NERDataset(test_data, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# 5. Configurar y entrenar el modelo\n",
    "print(\"\\nConfigurando y entrenando el modelo MobileBERT...\")\n",
    "\n",
    "# Aumentar significativamente los pesos para las clases minoritarias\n",
    "total_labels = 5450 + 169 + 132 + 44 + 28  # Total de todas las etiquetas\n",
    "\n",
    "# Opción más agresiva para los pesos\n",
    "class_weights = [\n",
    "    0.3,                         # O (casi ignorado)\n",
    "    total_labels / 143 * 3.0,     # B-PRODUCT (peso extremadamente alto)\n",
    "    total_labels / 28 * 3.0,     # I-PRODUCT (peso extremadamente alto)\n",
    "    total_labels / 194 * 9.0,     # B-ATTRIBUTE (peso extremadamente alto)\n",
    "    total_labels / 38 * 9.0      # I-ATTRIBUTE (peso extremadamente alto)\n",
    "]\n",
    "# Limitar los pesos máximos, pero permitir valores más altos\n",
    "class_weights = [min(weight, 50.0) for weight in class_weights]\n",
    "\n",
    "# Cargar modelo base\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(unique_tags),\n",
    "    id2label=id2tag,\n",
    "    label2id=tag2id\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Configurar argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.03,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",  # Cambiado a F1 para enfocarse en clases minoritarias\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, 'logs'),\n",
    ")\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "# Definir data collator personalizado con pesos de clase\n",
    "class WeightedDataCollator(DataCollatorForTokenClassification):\n",
    "    def __init__(self, tokenizer, class_weights, label_name=\"labels\"):\n",
    "        super().__init__(tokenizer=tokenizer)\n",
    "        self.class_weights = torch.tensor(class_weights)\n",
    "        self.label_name = label_name\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch = super().__call__(features)\n",
    "        return batch\n",
    "\n",
    "# Métrica personalizada para evaluación que incluye F1\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Eliminar tokens especiales y padding\n",
    "    true_predictions = [\n",
    "        [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Calcular métricas usando seqeval\n",
    "    report = classification_report(true_labels, true_predictions, output_dict=True)\n",
    "    \n",
    "    # Formatear para el trainer\n",
    "    results = {\n",
    "        'accuracy': report['micro avg']['precision'],\n",
    "        'precision': report['macro avg']['precision'],\n",
    "        'recall': report['macro avg']['recall'],\n",
    "        'f1': report['macro avg']['f1-score']\n",
    "    }\n",
    "    \n",
    "    # Añadir métricas por clase\n",
    "    for key in report:\n",
    "        if key not in ['macro avg', 'micro avg', 'weighted avg']:\n",
    "            results[f\"{key}_f1\"] = report[key]['f1-score']\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Inicializar trainer con loss_fct personalizado\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Tensor de pesos\n",
    "        weight = torch.tensor(class_weights, device=logits.device)\n",
    "        \n",
    "        # Calcular probabilidades\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        logits_view = logits.view(-1, model.config.num_labels)\n",
    "        labels_view = labels.view(-1)\n",
    "        \n",
    "        # Seleccionar probabilidades para las clases correctas\n",
    "        mask = labels_view != -100\n",
    "        valid_labels = labels_view[mask]\n",
    "        valid_logits = logits_view[mask]\n",
    "        \n",
    "        # Focal loss manual\n",
    "        gamma = 1.5\n",
    "        ce_loss = torch.nn.CrossEntropyLoss(weight=weight, reduction='none')\n",
    "        ce = ce_loss(valid_logits, valid_labels)\n",
    "        \n",
    "        pt = torch.exp(-ce)\n",
    "        focal_loss = ((1 - pt) ** gamma) * ce\n",
    "        \n",
    "        return (focal_loss.mean(), outputs) if return_outputs else focal_loss.mean()\n",
    "\n",
    "# Usar el CustomTrainer en lugar del Trainer estándar\n",
    "# Inicializar trainer con loss_fct personalizado\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp\\ipykernel_12088\\3984830881.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando entrenamiento...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27744e183fed4adabbab50f00d61d646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c7e6384bc741ba86a178f0b1989a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3114899694919586, 'eval_accuracy': 0.417910447761194, 'eval_precision': 0.3819047619047619, 'eval_recall': 0.358678955453149, 'eval_f1': 0.36904761904761907, 'eval_ATTRIBUTE_f1': 0.21428571428571427, 'eval_ATTRIBUTE_precision': 0.24, 'eval_ATTRIBUTE_recall': 0.1935483870967742, 'eval_PRODUCT_f1': 0.5238095238095238, 'eval_PRODUCT_precision': 0.5238095238095238, 'eval_PRODUCT_recall': 0.5238095238095238, 'eval_runtime': 2.9585, 'eval_samples_per_second': 16.563, 'eval_steps_per_second': 2.028, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64befbf246fc4b7ca7db8f74ba239178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21792076528072357, 'eval_accuracy': 0.5633802816901409, 'eval_precision': 0.5399193548387097, 'eval_recall': 0.5226574500768049, 'eval_f1': 0.5310778914240756, 'eval_ATTRIBUTE_f1': 0.35483870967741943, 'eval_ATTRIBUTE_precision': 0.3548387096774194, 'eval_ATTRIBUTE_recall': 0.3548387096774194, 'eval_PRODUCT_f1': 0.7073170731707318, 'eval_PRODUCT_precision': 0.725, 'eval_PRODUCT_recall': 0.6904761904761905, 'eval_runtime': 3.2509, 'eval_samples_per_second': 15.073, 'eval_steps_per_second': 1.846, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc142ea2b81448c95477e5097bbf21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20430731773376465, 'eval_accuracy': 0.573170731707317, 'eval_precision': 0.562015503875969, 'eval_recall': 0.6144393241167435, 'eval_f1': 0.5857142857142857, 'eval_ATTRIBUTE_f1': 0.3714285714285714, 'eval_ATTRIBUTE_precision': 0.3333333333333333, 'eval_ATTRIBUTE_recall': 0.41935483870967744, 'eval_PRODUCT_f1': 0.8, 'eval_PRODUCT_precision': 0.7906976744186046, 'eval_PRODUCT_recall': 0.8095238095238095, 'eval_runtime': 3.3366, 'eval_samples_per_second': 14.686, 'eval_steps_per_second': 1.798, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3e24fa3ed5428a890dfdb85f1facd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2287573516368866, 'eval_accuracy': 0.5949367088607594, 'eval_precision': 0.6188630490956072, 'eval_recall': 0.6228878648233487, 'eval_f1': 0.612959112959113, 'eval_ATTRIBUTE_f1': 0.4054054054054055, 'eval_ATTRIBUTE_precision': 0.3488372093023256, 'eval_ATTRIBUTE_recall': 0.4838709677419355, 'eval_PRODUCT_f1': 0.8205128205128205, 'eval_PRODUCT_precision': 0.8888888888888888, 'eval_PRODUCT_recall': 0.7619047619047619, 'eval_runtime': 3.2092, 'eval_samples_per_second': 15.269, 'eval_steps_per_second': 1.87, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b89a3ec8fa74f3bb5bf801a55adacd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23773092031478882, 'eval_accuracy': 0.6025641025641025, 'eval_precision': 0.625, 'eval_recall': 0.618663594470046, 'eval_f1': 0.6148577449947312, 'eval_ATTRIBUTE_f1': 0.3835616438356164, 'eval_ATTRIBUTE_precision': 0.3333333333333333, 'eval_ATTRIBUTE_recall': 0.45161290322580644, 'eval_PRODUCT_f1': 0.8461538461538461, 'eval_PRODUCT_precision': 0.9166666666666666, 'eval_PRODUCT_recall': 0.7857142857142857, 'eval_runtime': 3.0671, 'eval_samples_per_second': 15.976, 'eval_steps_per_second': 1.956, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea2a7786aea4bb99daffb5c2a59ac5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2715701758861542, 'eval_accuracy': 0.703125, 'eval_precision': 0.6581581581581581, 'eval_recall': 0.5779569892473119, 'eval_f1': 0.6154517677869926, 'eval_ATTRIBUTE_f1': 0.3448275862068965, 'eval_ATTRIBUTE_precision': 0.37037037037037035, 'eval_ATTRIBUTE_recall': 0.3225806451612903, 'eval_PRODUCT_f1': 0.8860759493670887, 'eval_PRODUCT_precision': 0.9459459459459459, 'eval_PRODUCT_recall': 0.8333333333333334, 'eval_runtime': 6.6943, 'eval_samples_per_second': 7.32, 'eval_steps_per_second': 0.896, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69815d02b028449f8ac7767be53a72bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2751080393791199, 'eval_accuracy': 0.5833333333333334, 'eval_precision': 0.5833333333333334, 'eval_recall': 0.6424731182795699, 'eval_f1': 0.6084474885844748, 'eval_ATTRIBUTE_f1': 0.3835616438356164, 'eval_ATTRIBUTE_precision': 0.3333333333333333, 'eval_ATTRIBUTE_recall': 0.45161290322580644, 'eval_PRODUCT_f1': 0.8333333333333334, 'eval_PRODUCT_precision': 0.8333333333333334, 'eval_PRODUCT_recall': 0.8333333333333334, 'eval_runtime': 3.5154, 'eval_samples_per_second': 13.939, 'eval_steps_per_second': 1.707, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8e08b06dc94bd0a37813f33d4e645b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27534908056259155, 'eval_accuracy': 0.6025641025641025, 'eval_precision': 0.6025641025641025, 'eval_recall': 0.618663594470046, 'eval_f1': 0.6074074074074074, 'eval_ATTRIBUTE_f1': 0.4, 'eval_ATTRIBUTE_precision': 0.358974358974359, 'eval_ATTRIBUTE_recall': 0.45161290322580644, 'eval_PRODUCT_f1': 0.8148148148148148, 'eval_PRODUCT_precision': 0.8461538461538461, 'eval_PRODUCT_recall': 0.7857142857142857, 'eval_runtime': 3.2505, 'eval_samples_per_second': 15.075, 'eval_steps_per_second': 1.846, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21f2ebf42f04508a1c82e3e8f3a7c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28308719396591187, 'eval_accuracy': 0.6052631578947368, 'eval_precision': 0.5987525987525988, 'eval_recall': 0.6025345622119815, 'eval_f1': 0.5985838779956427, 'eval_ATTRIBUTE_f1': 0.38235294117647056, 'eval_ATTRIBUTE_precision': 0.35135135135135137, 'eval_ATTRIBUTE_recall': 0.41935483870967744, 'eval_PRODUCT_f1': 0.8148148148148148, 'eval_PRODUCT_precision': 0.8461538461538461, 'eval_PRODUCT_recall': 0.7857142857142857, 'eval_runtime': 3.0294, 'eval_samples_per_second': 16.175, 'eval_steps_per_second': 1.981, 'epoch': 9.0}\n",
      "{'train_runtime': 986.9814, 'train_samples_per_second': 3.565, 'train_steps_per_second': 0.401, 'train_loss': 0.1339192245945786, 'epoch': 9.0}\n",
      "Modelo guardado en: C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\products_attributes\\final_model\n",
      "\n",
      "Evaluando el modelo en el conjunto de prueba...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1be58d8991e438d8fa30130f4e0f01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados de evaluación:\n",
      "eval_loss: 0.2685\n",
      "eval_accuracy: 0.6778\n",
      "eval_precision: 0.7176\n",
      "eval_recall: 0.7147\n",
      "eval_f1: 0.7145\n",
      "eval_ATTRIBUTE_f1: 0.5490\n",
      "eval_ATTRIBUTE_precision: 0.5185\n",
      "eval_ATTRIBUTE_recall: 0.5833\n",
      "eval_PRODUCT_f1: 0.8800\n",
      "eval_PRODUCT_precision: 0.9167\n",
      "eval_PRODUCT_recall: 0.8462\n",
      "eval_runtime: 3.0659\n",
      "eval_samples_per_second: 15.9820\n",
      "eval_steps_per_second: 1.9570\n",
      "epoch: 9.0000\n"
     ]
    }
   ],
   "source": [
    "# Métrica personalizada para evaluación\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Eliminar tokens especiales y padding\n",
    "    true_predictions = [\n",
    "        [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Calcular métricas usando seqeval\n",
    "    report = classification_report(true_labels, true_predictions, output_dict=True)\n",
    "    \n",
    "    # Formatear para el trainer\n",
    "    results = {\n",
    "        'accuracy': report['micro avg']['precision'],\n",
    "        'precision': report['macro avg']['precision'],\n",
    "        'recall': report['macro avg']['recall'],\n",
    "        'f1': report['macro avg']['f1-score']\n",
    "    }\n",
    "    \n",
    "    # Añadir métricas por clase\n",
    "    for key in report:\n",
    "        if key not in ['macro avg', 'micro avg', 'weighted avg']:\n",
    "            results[f\"{key}_f1\"] = report[key]['f1-score']\n",
    "            results[f\"{key}_precision\"] = report[key]['precision']\n",
    "            results[f\"{key}_recall\"] = report[key]['recall']\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Inicializar el entrenador\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "print(\"Comenzando entrenamiento...\")\n",
    "trainer.train()\n",
    "\n",
    "# Guardar el modelo final\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, 'final_model'))\n",
    "print(f\"Modelo guardado en: {os.path.join(OUTPUT_DIR, 'final_model')}\")\n",
    "\n",
    "# 6. Evaluación final\n",
    "print(\"\\nEvaluando el modelo en el conjunto de prueba...\")\n",
    "results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# Guardar resultados\n",
    "with open(os.path.join(OUTPUT_DIR, 'evaluation_results.json'), 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"\\nResultados de evaluación:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Función para extraer productos y atributos de texto\n",
    "def predict_entities(text, model, tokenizer, tag2id, id2tag, max_length=128):\n",
    "    \"\"\"\n",
    "    Predice entidades (productos y atributos) en un texto usando el modelo entrenado.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a analizar\n",
    "        model: Modelo MobileBERT entrenado\n",
    "        tokenizer: Tokenizador MobileBERT\n",
    "        tag2id: Mapeo de etiquetas a IDs\n",
    "        id2tag: Mapeo de IDs a etiquetas\n",
    "        max_length: Longitud máxima de secuencia\n",
    "        \n",
    "    Returns:\n",
    "        dict: Diccionario con productos y atributos detectados\n",
    "    \"\"\"\n",
    "    # Tokenizar el texto\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    \n",
    "    # Hacer la predicción\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded)\n",
    "    \n",
    "    # Obtener etiquetas predichas\n",
    "    predictions = torch.argmax(outputs.logits, axis=2).squeeze().tolist()\n",
    "    \n",
    "    # Obtener tokens originales\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "    \n",
    "    # Reconstruir entidades\n",
    "    products = []\n",
    "    attributes = []\n",
    "    \n",
    "    current_entity = []\n",
    "    current_type = None\n",
    "    \n",
    "    # Procesamos token por token\n",
    "    for token, pred_id in zip(tokens, predictions):\n",
    "        # Ignorar tokens especiales y continuaciones de subwords\n",
    "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token] or token.startswith(\"##\"):\n",
    "            if current_entity and current_type:\n",
    "                entity_text = \" \".join(current_entity).replace(\" ##\", \"\")\n",
    "                if current_type == \"PRODUCT\":\n",
    "                    products.append(entity_text)\n",
    "                elif current_type == \"ATTRIBUTE\":\n",
    "                    attributes.append(entity_text)\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "            continue\n",
    "        \n",
    "        # Obtener etiqueta predicha\n",
    "        pred_tag = id2tag.get(pred_id, \"O\")\n",
    "        \n",
    "        # Procesar según etiqueta\n",
    "        if pred_tag.startswith(\"B-\"):\n",
    "            # Si ya teníamos una entidad en curso, la guardamos\n",
    "            if current_entity and current_type:\n",
    "                entity_text = \" \".join(current_entity).replace(\" ##\", \"\")\n",
    "                if current_type == \"PRODUCT\":\n",
    "                    products.append(entity_text)\n",
    "                elif current_type == \"ATTRIBUTE\":\n",
    "                    attributes.append(entity_text)\n",
    "            \n",
    "            # Comenzar nueva entidad\n",
    "            current_entity = [token]\n",
    "            current_type = pred_tag[2:]  # Quitar el \"B-\"\n",
    "            \n",
    "        elif pred_tag.startswith(\"I-\") and current_entity:\n",
    "            # Asegurarnos de que estamos continuando el mismo tipo de entidad\n",
    "            if pred_tag[2:] == current_type:\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "                # Si cambia el tipo, guardamos la anterior y comenzamos una nueva\n",
    "                entity_text = \" \".join(current_entity).replace(\" ##\", \"\")\n",
    "                if current_type == \"PRODUCT\":\n",
    "                    products.append(entity_text)\n",
    "                elif current_type == \"ATTRIBUTE\":\n",
    "                    attributes.append(entity_text)\n",
    "                \n",
    "                current_entity = [token]\n",
    "                current_type = pred_tag[2:]\n",
    "                \n",
    "        elif pred_tag == \"O\" and current_entity:\n",
    "            # Fin de entidad\n",
    "            entity_text = \" \".join(current_entity).replace(\" ##\", \"\")\n",
    "            if current_type == \"PRODUCT\":\n",
    "                products.append(entity_text)\n",
    "            elif current_type == \"ATTRIBUTE\":\n",
    "                attributes.append(entity_text)\n",
    "            \n",
    "            current_entity = []\n",
    "            current_type = None\n",
    "    \n",
    "    # No olvidar la última entidad si quedó alguna\n",
    "    if current_entity and current_type:\n",
    "        entity_text = \" \".join(current_entity).replace(\" ##\", \"\")\n",
    "        if current_type == \"PRODUCT\":\n",
    "            products.append(entity_text)\n",
    "        elif current_type == \"ATTRIBUTE\":\n",
    "            attributes.append(entity_text)\n",
    "    \n",
    "    return {\n",
    "        \"products\": list(set(products)),  # Eliminar duplicados\n",
    "        \"attributes\": list(set(attributes))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo de uso del modelo:\n",
      "\n",
      "Texto: I love this foundation because it gives me a natural finish and doesn't crease all day.\n",
      "Productos detectados: ['foundation']\n",
      "Atributos detectados: ['natural finish']\n",
      "\n",
      "Texto: The concealer is great for dark circles and has amazing coverage.\n",
      "Productos detectados: ['conceal']\n",
      "Atributos detectados: []\n",
      "\n",
      "Texto: This highlighter gives me a beautiful glow and lasts all day.\n",
      "Productos detectados: ['highlight']\n",
      "Atributos detectados: ['glow']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Ejemplo de uso\n",
    "print(\"\\nEjemplo de uso del modelo:\")\n",
    "\n",
    "# Cargar modelo y mapeo de etiquetas\n",
    "model_path = os.path.join(OUTPUT_DIR, 'final_model')\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'tag_mapping.json'), 'r') as f:\n",
    "    tag_mapping = json.load(f)\n",
    "    tag2id = tag_mapping['tag2id']\n",
    "    id2tag = {int(k): v for k, v in tag_mapping['id2tag'].items()}\n",
    "\n",
    "# Probar con algunos ejemplos\n",
    "test_examples = [\n",
    "    \"I love this foundation because it gives me a natural finish and doesn't crease all day.\",\n",
    "    \"The concealer is great for dark circles and has amazing coverage.\",\n",
    "    \"This highlighter gives me a beautiful glow and lasts all day.\"\n",
    "]\n",
    "\n",
    "for example in test_examples:\n",
    "    results = predict_entities(example, model, tokenizer, tag2id, id2tag)\n",
    "    print(f\"\\nTexto: {example}\")\n",
    "    print(f\"Productos detectados: {results['products']}\")\n",
    "    print(f\"Atributos detectados: {results['attributes']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo completo: C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\results_brand_detection.xlsx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2da0c8f2f7546be861557a64b048087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando transcripciones:   0%|          | 0/6781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset procesado guardado en: C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\products_attributes\\results\\sentences_transcriptions_processed.xlsx\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 9. Código para procesar todo el dataset original\n",
    "def process_full_dataset(input_file, output_file, model, tokenizer, tag2id, id2tag):\n",
    "    \"\"\"\n",
    "    Procesa todo el dataset original y genera un nuevo archivo con productos y atributos detectados.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Ruta al archivo Excel original\n",
    "        output_file (str): Ruta donde guardar el archivo procesado\n",
    "        model: Modelo MobileBERT entrenado\n",
    "        tokenizer: Tokenizador MobileBERT\n",
    "        tag2id: Mapeo de etiquetas a IDs\n",
    "        id2tag: Mapeo de IDs a etiquetas\n",
    "    \"\"\"\n",
    "    print(f\"Procesando archivo completo: {input_file}\")\n",
    "    \n",
    "    # Cargar el dataset original\n",
    "    df_original = pd.read_excel(input_file)\n",
    "    \n",
    "    # Columnas para resultados\n",
    "    df_original['detected_products'] = \"\"\n",
    "    df_original['detected_attributes'] = \"\"\n",
    "    \n",
    "    # Procesar cada fila\n",
    "    for i, row in tqdm(df_original.iterrows(), total=len(df_original), desc=\"Procesando transcripciones\"):\n",
    "        try:\n",
    "            text = row['transcription']\n",
    "            if pd.isna(text) or text.strip() == \"\":\n",
    "                continue\n",
    "                \n",
    "            # Predecir entidades\n",
    "            results = predict_entities(text, model, tokenizer, tag2id, id2tag)\n",
    "            \n",
    "            # Guardar resultados\n",
    "            df_original.at[i, 'detected_products'] = \", \".join(results['products'])\n",
    "            df_original.at[i, 'detected_attributes'] = \", \".join(results['attributes'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando fila {i}: {str(e)}\")\n",
    "    \n",
    "    # Guardar el dataset procesado\n",
    "    df_original.to_excel(output_file, index=False)\n",
    "    print(f\"Dataset procesado guardado en: {output_file}\")\n",
    "\n",
    "# Ejemplo de uso para procesar todo el dataset\n",
    "# Descomenta estas líneas cuando quieras procesar todo el archivo\n",
    "full_dataset_path = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\results_brand_detection.xlsx\"\n",
    "output_path = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\products_attributes\\results\\sentences_transcriptions_processed.xlsx\"\n",
    "process_full_dataset(full_dataset_path, output_path, model, tokenizer, tag2id, id2tag)\n",
    "\n",
    "\n",
    "# 10. Visualización de pérdida durante entrenamiento\n",
    "train_history = trainer.state.log_history\n",
    "\n",
    "# Extraer valores de pérdida\n",
    "train_losses = [x.get('loss') for x in train_history if 'loss' in x and 'eval_loss' not in x]\n",
    "train_steps = [x.get('step') for x in train_history if 'loss' in x and 'eval_loss' not in x]\n",
    "eval_losses = [x.get('eval_loss') for x in train_history if 'eval_loss' in x]\n",
    "eval_steps = [x.get('step') for x in train_history if 'eval_loss' in x]\n",
    "\n",
    "# Graficar pérdida\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_steps, train_losses, label='Training Loss')\n",
    "plt.plot(eval_steps, eval_losses, label='Validation Loss', marker='o')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'loss_curves.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado en: C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\detection_results\\results_detections.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_urlvideo</th>\n",
       "      <th>transcription</th>\n",
       "      <th>brand_detected</th>\n",
       "      <th>match_text</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>context_text</th>\n",
       "      <th>detected_products</th>\n",
       "      <th>detected_attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.tiktok.com/@aliciarev/video/728025...</td>\n",
       "      <td>I use the Genifit from Lancome, which costs on...</td>\n",
       "      <td>Lancôme</td>\n",
       "      <td>e Genifit from Lancome, which costs o</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>It leaves your skin glowing, not the next thin...</td>\n",
       "      <td>calming clean, eye con, tone, clean, cleansing...</td>\n",
       "      <td>worth, expensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.tiktok.com/@aliciarev/video/728025...</td>\n",
       "      <td>I also use Lancome's Genifit serum.</td>\n",
       "      <td>Lancôme</td>\n",
       "      <td>I also use Lancome's Genifit seru</td>\n",
       "      <td>0.8674</td>\n",
       "      <td>positive</td>\n",
       "      <td>I use the Genifit from Lancome, which costs on...</td>\n",
       "      <td>gen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://www.tiktok.com/@aliciarev/video/728025...</td>\n",
       "      <td>It's Eve Pound, the blue one from Drunk Elephant.</td>\n",
       "      <td>Drunk Elephant</td>\n",
       "      <td>blue one from Drunk Elephant.</td>\n",
       "      <td>0.7783</td>\n",
       "      <td>positive</td>\n",
       "      <td>And since I like to use really thick things at...</td>\n",
       "      <td>moist, cream</td>\n",
       "      <td>moist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.tiktok.com/@aliciarev/video/728025...</td>\n",
       "      <td>Then I put on the eyebrow serum, which is Bene...</td>\n",
       "      <td>Benefit Cosmetics</td>\n",
       "      <td>erum, which is Benefit's Huba Brow.</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>positive</td>\n",
       "      <td>They help me so that the nuclear white isn't s...</td>\n",
       "      <td>eyebrow serum, white</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://www.tiktok.com/@aliciarev/video/728025...</td>\n",
       "      <td>And finally, for the lips, the Laneige lip sle...</td>\n",
       "      <td>Laneige</td>\n",
       "      <td>the lips, the Laneige lip sleeping m</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>positive</td>\n",
       "      <td>It costs 71 euros. Well, it works really well....</td>\n",
       "      <td>sleeping mask</td>\n",
       "      <td>chocolate, generous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6769</th>\n",
       "      <td>https://www.tiktok.com/@aleshadows_/video/7454...</td>\n",
       "      <td>The one from the Kerr line from Sephora Collec...</td>\n",
       "      <td>SEPHORA COLLECTION</td>\n",
       "      <td>Kerr line from Sephora Collection.</td>\n",
       "      <td>0.8100</td>\n",
       "      <td>positive</td>\n",
       "      <td>Let's go with the cream contour or bronzer. Of...</td>\n",
       "      <td>bronze</td>\n",
       "      <td>low cost, amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6771</th>\n",
       "      <td>https://www.tiktok.com/@aleshadows_/video/7454...</td>\n",
       "      <td>And again Make Up by Mario, but this time this...</td>\n",
       "      <td>Make Up By Mario</td>\n",
       "      <td>And again Make Up by Mario, but this time</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>positive</td>\n",
       "      <td>The one from the Kerr line from Sephora Collec...</td>\n",
       "      <td>highlight, bal</td>\n",
       "      <td>gorgeous, shade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6776</th>\n",
       "      <td>https://www.tiktok.com/@aleshadows_/video/7454...</td>\n",
       "      <td>A classic every year is the Pingasem from Char...</td>\n",
       "      <td>Charlotte Tilbury</td>\n",
       "      <td>Pingasem from Charlotte Tilbury.</td>\n",
       "      <td>0.8519</td>\n",
       "      <td>positive</td>\n",
       "      <td>I have talked about it a thousand times but it...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>classic, perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6777</th>\n",
       "      <td>https://www.tiktok.com/@aleshadows_/video/7454...</td>\n",
       "      <td>The stick blush from Make Up by Mario in the s...</td>\n",
       "      <td>Make Up By Mario</td>\n",
       "      <td>ick blush from Make Up by Mario in the shade P</td>\n",
       "      <td>0.9052</td>\n",
       "      <td>positive</td>\n",
       "      <td>And last but not least in this video, liquid c...</td>\n",
       "      <td>stick blush, bronze</td>\n",
       "      <td>shade, blush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6779</th>\n",
       "      <td>https://www.tiktok.com/@aleshadows_/video/7454...</td>\n",
       "      <td>And I promise you that Make Up by Mario has no...</td>\n",
       "      <td>Make Up By Mario</td>\n",
       "      <td>omise you that Make Up by Mario has not paid m</td>\n",
       "      <td>0.8151</td>\n",
       "      <td>positive</td>\n",
       "      <td>The stick blush from Make Up by Mario in the s...</td>\n",
       "      <td>powder, blush veil</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1181 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            id_urlvideo  \\\n",
       "15    https://www.tiktok.com/@aliciarev/video/728025...   \n",
       "17    https://www.tiktok.com/@aliciarev/video/728025...   \n",
       "25    https://www.tiktok.com/@aliciarev/video/728025...   \n",
       "30    https://www.tiktok.com/@aliciarev/video/728025...   \n",
       "34    https://www.tiktok.com/@aliciarev/video/728025...   \n",
       "...                                                 ...   \n",
       "6769  https://www.tiktok.com/@aleshadows_/video/7454...   \n",
       "6771  https://www.tiktok.com/@aleshadows_/video/7454...   \n",
       "6776  https://www.tiktok.com/@aleshadows_/video/7454...   \n",
       "6777  https://www.tiktok.com/@aleshadows_/video/7454...   \n",
       "6779  https://www.tiktok.com/@aleshadows_/video/7454...   \n",
       "\n",
       "                                          transcription      brand_detected  \\\n",
       "15    I use the Genifit from Lancome, which costs on...             Lancôme   \n",
       "17                  I also use Lancome's Genifit serum.             Lancôme   \n",
       "25    It's Eve Pound, the blue one from Drunk Elephant.      Drunk Elephant   \n",
       "30    Then I put on the eyebrow serum, which is Bene...   Benefit Cosmetics   \n",
       "34    And finally, for the lips, the Laneige lip sle...             Laneige   \n",
       "...                                                 ...                 ...   \n",
       "6769  The one from the Kerr line from Sephora Collec...  SEPHORA COLLECTION   \n",
       "6771  And again Make Up by Mario, but this time this...    Make Up By Mario   \n",
       "6776  A classic every year is the Pingasem from Char...   Charlotte Tilbury   \n",
       "6777  The stick blush from Make Up by Mario in the s...    Make Up By Mario   \n",
       "6779  And I promise you that Make Up by Mario has no...    Make Up By Mario   \n",
       "\n",
       "                                          match_text  sentiment_score  \\\n",
       "15             e Genifit from Lancome, which costs o           0.0000   \n",
       "17                 I also use Lancome's Genifit seru           0.8674   \n",
       "25                     blue one from Drunk Elephant.           0.7783   \n",
       "30               erum, which is Benefit's Huba Brow.           0.7189   \n",
       "34              the lips, the Laneige lip sleeping m           0.8805   \n",
       "...                                              ...              ...   \n",
       "6769              Kerr line from Sephora Collection.           0.8100   \n",
       "6771       And again Make Up by Mario, but this time           0.9725   \n",
       "6776                Pingasem from Charlotte Tilbury.           0.8519   \n",
       "6777  ick blush from Make Up by Mario in the shade P           0.9052   \n",
       "6779  omise you that Make Up by Mario has not paid m           0.8151   \n",
       "\n",
       "     sentiment_label                                       context_text  \\\n",
       "15           neutral  It leaves your skin glowing, not the next thin...   \n",
       "17          positive  I use the Genifit from Lancome, which costs on...   \n",
       "25          positive  And since I like to use really thick things at...   \n",
       "30          positive  They help me so that the nuclear white isn't s...   \n",
       "34          positive  It costs 71 euros. Well, it works really well....   \n",
       "...              ...                                                ...   \n",
       "6769        positive  Let's go with the cream contour or bronzer. Of...   \n",
       "6771        positive  The one from the Kerr line from Sephora Collec...   \n",
       "6776        positive  I have talked about it a thousand times but it...   \n",
       "6777        positive  And last but not least in this video, liquid c...   \n",
       "6779        positive  The stick blush from Make Up by Mario in the s...   \n",
       "\n",
       "                                      detected_products  detected_attributes  \n",
       "15    calming clean, eye con, tone, clean, cleansing...     worth, expensive  \n",
       "17                                                  gen                  NaN  \n",
       "25                                         moist, cream                moist  \n",
       "30                                 eyebrow serum, white                  NaN  \n",
       "34                                        sleeping mask  chocolate, generous  \n",
       "...                                                 ...                  ...  \n",
       "6769                                             bronze    low cost, amazing  \n",
       "6771                                     highlight, bal      gorgeous, shade  \n",
       "6776                                                NaN     classic, perfect  \n",
       "6777                                stick blush, bronze         shade, blush  \n",
       "6779                                 powder, blush veil                  NaN  \n",
       "\n",
       "[1181 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Siguiente paso tengo que coger el archivo este que me devuelve y agrupar las caracteristicas por menciones cercanas. \n",
    "# Poner también que si no detecta marca lo deje en blanco no q no ponga no se ha detectado, pq luego el MBA se lia.\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file_path = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\products_attributes\\results\\sentences_transcriptions_processed.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df['brand_detected'] = df['brand_detected'].replace(\"No se detectó marca\", \"\")\n",
    "\n",
    "# Agrupar el DataFrame por URL\n",
    "grouped = df.groupby('id_urlvideo')\n",
    "\n",
    "# Nuevo DataFrame para almacenar resultados\n",
    "result_df = df.copy()\n",
    "\n",
    "# Para cada grupo de URL\n",
    "for url, group in grouped:\n",
    "    # Identificar filas con marca\n",
    "    marca_indices = group[group['brand_detected'] != \"\"].index.tolist()\n",
    "    \n",
    "    if not marca_indices:  # Si no hay marcas en este grupo, continuar\n",
    "        continue\n",
    "    \n",
    "    # Para cada fila en el grupo\n",
    "    for idx in group.index:\n",
    "        # Si la fila ya tiene marca, mantener sus productos y atributos\n",
    "        if idx in marca_indices:\n",
    "            continue\n",
    "        \n",
    "        # Encontrar el índice con marca más cercano\n",
    "        closest_marca_idx = min(marca_indices, key=lambda x: abs(x - idx))\n",
    "        \n",
    "        # Productos detectados en la fila actual\n",
    "        productos = group.loc[idx, 'detected_products']\n",
    "        atributos = group.loc[idx, 'detected_attributes']\n",
    "        \n",
    "        # Si hay productos, añadirlos a la fila con marca más cercana\n",
    "        if productos and not pd.isna(productos):\n",
    "            # Obtener productos actuales de la fila con marca\n",
    "            current_products = result_df.loc[closest_marca_idx, 'detected_products']\n",
    "            if pd.isna(current_products) or current_products == \"\":\n",
    "                result_df.loc[closest_marca_idx, 'detected_products'] = productos\n",
    "            else:\n",
    "                # Combinar evitando duplicados\n",
    "                all_products = set(current_products.split(\", \") + productos.split(\", \"))\n",
    "                all_products = {p for p in all_products if p}  # Eliminar cadenas vacías\n",
    "                result_df.loc[closest_marca_idx, 'detected_products'] = \", \".join(all_products)\n",
    "        \n",
    "        # Si hay atributos, añadirlos a la fila con marca más cercana\n",
    "        if atributos and not pd.isna(atributos):\n",
    "            # Obtener atributos actuales de la fila con marca\n",
    "            current_attrs = result_df.loc[closest_marca_idx, 'detected_attributes']\n",
    "            if pd.isna(current_attrs) or current_attrs == \"\":\n",
    "                result_df.loc[closest_marca_idx, 'detected_attributes'] = atributos\n",
    "            else:\n",
    "                # Combinar evitando duplicados\n",
    "                all_attrs = set(current_attrs.split(\", \") + atributos.split(\", \"))\n",
    "                all_attrs = {a for a in all_attrs if a}  # Eliminar cadenas vacías\n",
    "                result_df.loc[closest_marca_idx, 'detected_attributes'] = \", \".join(all_attrs)\n",
    "        \n",
    "        # Limpiar la fila original (sin marca)\n",
    "        result_df.loc[idx, 'detected_products'] = \"\"\n",
    "        result_df.loc[idx, 'detected_attributes'] = \"\"\n",
    "result_df = result_df[result_df['brand_detected'] != \"\"]\n",
    "# Guardar el resultado\n",
    "output_path = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\detection_results\\results_detections.xlsx\"\n",
    "result_df.to_excel(output_path, index=False)\n",
    "print(f\"Archivo guardado en: {output_path}\")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame corregido guardado en: C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\detection_results\\corrected_results_detections.xlsx\n",
      "\n",
      "Ejemplos de correcciones aplicadas:\n",
      "\n",
      "Ejemplo 1:\n",
      "  Productos originales: calming clean, eye con, tone, clean, cleansing oil\n",
      "  Productos corregidos: calming clean, eye con, tone, clean, cleansing oil\n",
      "  Atributos originales: worth, expensive\n",
      "  Atributos corregidos: worth, expensive\n",
      "\n",
      "Ejemplo 2:\n",
      "  Productos originales: gen\n",
      "  Productos corregidos: gen\n",
      "  Atributos originales: nan\n",
      "  Atributos corregidos: nan\n",
      "\n",
      "Ejemplo 3:\n",
      "  Productos originales: moist, cream\n",
      "  Productos corregidos: moist, cream\n",
      "  Atributos originales: moist\n",
      "  Atributos corregidos: moist\n",
      "\n",
      "Ejemplo 4:\n",
      "  Productos originales: eyebrow serum, white\n",
      "  Productos corregidos: eyebrow serum, white\n",
      "  Atributos originales: nan\n",
      "  Atributos corregidos: nan\n",
      "\n",
      "Ejemplo 5:\n",
      "  Productos originales: sleeping mask\n",
      "  Productos corregidos: sleeping mask\n",
      "  Atributos originales: chocolate, generous\n",
      "  Atributos corregidos: chocolate, generous\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import get_close_matches\n",
    "import re\n",
    "\n",
    "def extract_unique_terms(dataframe, column_name):\n",
    "    \"\"\"Función que saca los valores unicos de columnas con listas separadas por coma.\"\"\"\n",
    "    filtered_df = dataframe[~dataframe[column_name].isna()]\n",
    "    unique_values = set()\n",
    "    for value in filtered_df[column_name]:\n",
    "        if isinstance(value, str) and value.strip():\n",
    "            items = [item.strip() for item in value.split(',')]\n",
    "            unique_values.update([item for item in items if item])\n",
    "    \n",
    "    unique_list = sorted(list(unique_values))\n",
    "    return unique_list\n",
    "\n",
    "unique_products = extract_unique_terms(result_df, 'detected_products')\n",
    "unique_attributes = extract_unique_terms(result_df, 'detected_attributes')\n",
    "\n",
    "# Cargar el archivo con las categorías originales\n",
    "categories_path = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\data_bert\\labeled_sentences.xlsx\"\n",
    "categories_df = pd.read_excel(categories_path)\n",
    "product_column = categories_df.columns[2]  # columna de productos_detected\n",
    "attribute_column = categories_df.columns[3] # columna attributes_detected\n",
    "# Categorías únicas de los datos etiquetados\n",
    "def extract_product_categories(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # Dividir por comas primero\n",
    "    items = [item.strip() for item in text.split(',')]\n",
    "    products = []\n",
    "    \n",
    "    for item in items:\n",
    "        # Extraer el nombre del producto (ignorando la categoría entre paréntesis)\n",
    "        match = re.match(r'^(.*?)\\s*\\([^)]*\\)$', item.strip())\n",
    "        if match:\n",
    "            product = match.group(1).strip().lower()\n",
    "            if product:\n",
    "                products.append(product)\n",
    "        else:\n",
    "            # Si no hay paréntesis, usar el elemento completo\n",
    "            product = item.strip().lower()\n",
    "            if product:\n",
    "                products.append(product)\n",
    "    \n",
    "    return products\n",
    "\n",
    "def extract_attribute_categories(text):\n",
    "    \"\"\"Extraer Productos y Atributos Únicos sin parentesis y sin comas.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # Dividir por comas y limpiar\n",
    "    attributes = [attr.strip().lower() for attr in text.split(',')]\n",
    "    return [attr for attr in attributes if attr]\n",
    "\n",
    "all_product_categories = []\n",
    "for text in categories_df[product_column].dropna():\n",
    "    all_product_categories.extend(extract_product_categories(text))\n",
    "\n",
    "all_attribute_categories = []\n",
    "for text in categories_df[attribute_column].dropna():\n",
    "    all_attribute_categories.extend(extract_attribute_categories(text))\n",
    "\n",
    "unique_product_categories = sorted(set(all_product_categories))\n",
    "unique_attribute_categories = sorted(set(all_attribute_categories))\n",
    "\n",
    "def refined_assign_to_category(term, categories, similar_terms=None, min_term_length=3):\n",
    "    \"\"\"\n",
    "    Función refinada para asignar términos a categorías, conservando más nombres de productos.\n",
    "    \n",
    "    Args:\n",
    "        term (str): Término a asignar\n",
    "        categories (list): Lista de categorías disponibles\n",
    "        similar_terms (dict, optional): Diccionario de términos similares o correcciones manuales\n",
    "        min_term_length (int): Longitud mínima para considerar un término como válido\n",
    "    \n",
    "    Returns:\n",
    "        str: Categoría asignada\n",
    "    \"\"\"\n",
    "    # Limpiar el término\n",
    "    term_lower = term.lower().strip()\n",
    "    \n",
    "    # Verificar correcciones manuales primero\n",
    "    if similar_terms and term_lower in similar_terms:\n",
    "        return similar_terms[term_lower]\n",
    "    \n",
    "    # Intentar coincidencia exacta\n",
    "    if term_lower in categories:\n",
    "        return term_lower\n",
    "    \n",
    "    # Lista de términos que definitivamente no son productos/atributos\n",
    "    definite_non_products = ['-', '201', '+', '&', '/', '•']\n",
    "    if term_lower in definite_non_products:\n",
    "        return \"other\"\n",
    "    \n",
    "    # Casos especiales con correcciones manuales\n",
    "    if term_lower == \"beauty blend\" or term_lower == \"beauty blender\":\n",
    "        return \"beauty blender\"\n",
    "    \n",
    "    # Para términos como \"backstage\", \"acid\", etc. que podrían ser nombres de productos\n",
    "    # Si tienen una longitud razonable, los mantenemos como están\n",
    "    if len(term_lower) >= min_term_length and not term_lower.isdigit():\n",
    "        # Verificar si es parte de una categoría conocida\n",
    "        for category in categories:\n",
    "            pattern = r'\\b' + re.escape(term_lower) + r'\\b'\n",
    "            if re.search(pattern, category):\n",
    "                return category\n",
    "        \n",
    "        # Si no es parte de una categoría pero parece un nombre válido, lo dejamos\n",
    "        return term_lower\n",
    "    \n",
    "    # Buscar coincidencias parciales priorizando términos más largos\n",
    "    sorted_categories = sorted(categories, key=len, reverse=True)\n",
    "    \n",
    "    for category in sorted_categories:\n",
    "        # Comprobar si la categoría es una palabra completa dentro del término\n",
    "        pattern = r'\\b' + re.escape(category) + r'\\b'\n",
    "        if re.search(pattern, term_lower):\n",
    "            return category\n",
    "    \n",
    "    # Si el término es corto pero significativo (como \"bb\" en \"bb cream\")\n",
    "    if term_lower in [\"bb\", \"cc\", \"dd\", \"spf\"] and len(term_lower) == 2:\n",
    "        return term_lower\n",
    "    \n",
    "    # Usar similitud de cadena con umbral más bajo para términos que parecen nombres\n",
    "    matches = get_close_matches(term_lower, categories, n=1, cutoff=0.7)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    \n",
    "    # Si parece un nombre de producto válido, lo conservamos\n",
    "    if len(term_lower) >= min_term_length and not term_lower.isdigit() and term_lower not in [\"the\", \"and\", \"for\", \"with\"]:\n",
    "        return term_lower\n",
    "    \n",
    "    # Si nada más funciona, devolver \"other\"\n",
    "    return \"other\"\n",
    "\n",
    "# Definir correcciones manuales específicas\n",
    "manual_corrections_products = {\n",
    "    \"beauty blend\": \"beauty blender\",\n",
    "    \"benefit mascara\": \"mascara\",\n",
    "    \"barrier cream\": \"cream\",\n",
    "    \"backstage\": \"backstage palette\", \n",
    "    \"acid\": \"acid treatment\",\n",
    "    \"bal\": \"balm\",\n",
    "    \"vase\":\"base\",\n",
    "    \"tan\": \"tanned\",\n",
    "    \"ta\": \"tanned\",\n",
    "    \"skin pri\":\"skin primer\",\n",
    "    \"settings break\":\"setting spray\",\n",
    "    \"pillow tail\": \"pillow talk\",\n",
    "    \"pillow\": \"pillow talk\",\n",
    "    \"pen\": \"pencil\",\n",
    "    \"orgasm\":\"pinkgasm\",\n",
    "    \"nose con\":\"contour\",\n",
    "    \"conceal\": \"concealer\",\n",
    "    \"condition\": \"concealer\",\n",
    "    \"con\":\"contour\",\n",
    "    \"nose con\":\"contour\",\n",
    "    \"pen\": \"pencil\",\n",
    "}\n",
    "\n",
    "manual_corrections_attributes = {\n",
    "    \"allergy tested\": \"allergy tested\",\n",
    "    \"amazing\": \"amazing\",\n",
    "    \"good\": \"good\",\n",
    "    \"four shades\":\"shade\",\n",
    "    \"sham\":\"shine\",\n",
    "    \"shadows\":\"shadow\",\n",
    "    \"settings break\":\"setting spray\",\n",
    "    \"pillow tail\": \"pillow talk\",\n",
    "    \"pillow\": \"pillow talk\",\n",
    "    \"orgasm\":\"pinkgasm\",\n",
    "\n",
    "}\n",
    "\n",
    "# Aplicar el mapeo refinado\n",
    "refined_product_mappings = {}\n",
    "for product in unique_products:\n",
    "    refined_product_mappings[product] = refined_assign_to_category(\n",
    "        product, \n",
    "        unique_product_categories, \n",
    "        manual_corrections_products\n",
    "    )\n",
    "\n",
    "refined_attribute_mappings = {}\n",
    "for attribute in unique_attributes:\n",
    "    refined_attribute_mappings[attribute] = refined_assign_to_category(\n",
    "        attribute, \n",
    "        unique_attribute_categories, \n",
    "        manual_corrections_attributes\n",
    "    )\n",
    "# Aplicar los mapeos refinados al DataFrame original\n",
    "def apply_mappings_to_dataframe(df, product_mappings, attribute_mappings):\n",
    "    \"\"\"\n",
    "    Aplica los mapeos de corrección a las columnas de productos y atributos del DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame original\n",
    "        product_mappings: Diccionario de mapeos para productos\n",
    "        attribute_mappings: Diccionario de mapeos para atributos\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con productos y atributos corregidos\n",
    "    \"\"\"\n",
    "    # Crear una copia del DataFrame para no modificar el original\n",
    "    corrected_df = df.copy()\n",
    "    \n",
    "    # Función para aplicar mapeos a una lista de términos separados por comas\n",
    "    def apply_mapping_to_list(value, mappings):\n",
    "        if not isinstance(value, str) or not value.strip():\n",
    "            return value\n",
    "        \n",
    "        # Dividir por comas\n",
    "        items = [item.strip() for item in value.split(',')]\n",
    "        \n",
    "        # Aplicar mapeo a cada término\n",
    "        corrected_items = []\n",
    "        for item in items:\n",
    "            if item:\n",
    "                # Si el ítem está en nuestro mapeo y no es \"other\", usarlo\n",
    "                mapped_item = mappings.get(item.lower(), item)\n",
    "                if mapped_item != \"other\":\n",
    "                    corrected_items.append(mapped_item)\n",
    "                # Si no está en nuestro mapeo, mantener el original\n",
    "                else:\n",
    "                    if len(item) >= 3:  # Mantener solo términos significativos\n",
    "                        corrected_items.append(item)\n",
    "        \n",
    "        # Unir los términos corregidos\n",
    "        return \", \".join(corrected_items) if corrected_items else \"\"\n",
    "    \n",
    "    # Aplicar correcciones a la columna de productos\n",
    "    if 'detected_products' in corrected_df.columns:\n",
    "        corrected_df['detected_products'] = corrected_df['detected_products'].apply(\n",
    "            lambda x: apply_mapping_to_list(x, product_mappings)\n",
    "        )\n",
    "    \n",
    "    # Aplicar correcciones a la columna de atributos\n",
    "    if 'detected_attributes' in corrected_df.columns:\n",
    "        corrected_df['detected_attributes'] = corrected_df['detected_attributes'].apply(\n",
    "            lambda x: apply_mapping_to_list(x, attribute_mappings)\n",
    "        )\n",
    "    \n",
    "    return corrected_df\n",
    "\n",
    "# Aplicar las correcciones\n",
    "corrected_result_df = apply_mappings_to_dataframe(result_df, refined_product_mappings, refined_attribute_mappings)\n",
    "\n",
    "# Guardar el DataFrame corregido en un nuevo Excel\n",
    "output_path = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Detection\\detection_results\\corrected_results_detections.xlsx\"\n",
    "corrected_result_df.to_excel(output_path, index=False)\n",
    "print(f\"DataFrame corregido guardado en: {output_path}\")\n",
    "\n",
    "# Verificar algunos ejemplos de correcciones\n",
    "print(\"\\nEjemplos de correcciones aplicadas:\")\n",
    "if len(result_df) > 0:\n",
    "    sample_indices = min(5, len(result_df))\n",
    "    for i in range(sample_indices):\n",
    "        original_products = result_df.iloc[i]['detected_products'] if 'detected_products' in result_df.columns else ''\n",
    "        corrected_products = corrected_result_df.iloc[i]['detected_products'] if 'detected_products' in corrected_result_df.columns else ''\n",
    "        \n",
    "        original_attributes = result_df.iloc[i]['detected_attributes'] if 'detected_attributes' in result_df.columns else ''\n",
    "        corrected_attributes = corrected_result_df.iloc[i]['detected_attributes'] if 'detected_attributes' in corrected_result_df.columns else ''\n",
    "        \n",
    "        print(f\"\\nEjemplo {i+1}:\")\n",
    "        print(f\"  Productos originales: {original_products}\")\n",
    "        print(f\"  Productos corregidos: {corrected_products}\")\n",
    "        print(f\"  Atributos originales: {original_attributes}\")\n",
    "        print(f\"  Atributos corregidos: {corrected_attributes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo CSV: C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\data\\clean_data\\sephora_website_cleaned.csv\n",
      "Procesando datos y extrayendo términos significativos por marca...\n",
      "Archivo guardado en: C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Kmeans\\data for kmeans\\brand_meaningful_terms_forkmeans.xlsx\n",
      "Total de marcas procesadas: 147\n",
      "\n",
      "Ejemplos de las primeras marcas procesadas:\n",
      "\n",
      "Marca: SEPHORA COLLECTION\n",
      "Promedio de precio: $18.91\n",
      "Número de productos: 142\n",
      "Términos significativos: vitamin e, caffeine, natural, concealer, limited-edition, brightening, mascara, primer, professional, tea tree, cream, full-coverage, hypoallergenic, powder, glossy, lip gloss, ceramides, niacinamide, plumping, metallic, alcohol-free, contour, lipstick, balm, highlighter, hyaluronic acid, vitamin c, bronzer, oil, lip balm\n",
      "\n",
      "Marca: Anastasia Beverly Hills\n",
      "Promedio de precio: $25.18\n",
      "Número de productos: 83\n",
      "Términos significativos: vitamin e, peptides, premium, natural, concealer, dermatologist-tested, mascara, primer, professional, cream, full-coverage, powder, shimmery, lip gloss, niacinamide, plumping, dewy, metallic, contour, lipstick, balm, highlighter, hyaluronic acid, bronzer, oil, eyeliner, foundation, serum, eyeshadow, hydrating\n",
      "\n",
      "Marca: tarte\n",
      "Promedio de precio: $28.54\n",
      "Número de productos: 82\n",
      "Términos significativos: vitamin e, peptides, natural, concealer, limited-edition, brightening, dermatologist-tested, mascara, primer, cream, full-coverage, hypoallergenic, powder, glossy, lip gloss, spf, antioxidants, niacinamide, plumping, dewy, metallic, contour, lipstick, balm, highlighter, hyaluronic acid, vitamin c, bronzer, oil, lip balm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Configurar rutas\n",
    "input_file = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\data\\clean_data\\sephora_website_cleaned.csv\"\n",
    "output_dir = r\"C:\\Users\\sandr\\Documents\\scrp_tiktok_tfg\\analysis\\Kmeans\\data for kmeans\"\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Leer el archivo CSV de Sephora\n",
    "print(f\"Leyendo archivo CSV: {input_file}\")\n",
    "sephora_df = pd.read_csv(input_file)\n",
    "\n",
    "# Definir términos específicos que caracterizan productos cosméticos (convertido a conjunto)\n",
    "cosmetic_specific_terms = {\n",
    "    # Tipo de producto\n",
    "    'foundation', 'concealer', 'powder', 'blush', 'bronzer', 'highlighter', \n",
    "    'eyeshadow', 'eyeliner', 'mascara', 'lipstick', 'lip gloss', 'lip balm', \n",
    "    'primer', 'setting spray', 'face mask', 'moisturizer', 'serum', 'toner', \n",
    "    'cleanser', 'eye cream', 'sunscreen', 'bb cream', 'cc cream', 'brow pencil',\n",
    "    'palette', 'contour', 'illuminator', 'sponge', 'beauty blender',\n",
    "    \n",
    "    # Texturas y acabados\n",
    "    'matte', 'dewy', 'glowy', 'radiant', 'luminous', 'shimmery', 'glossy', 'satin',\n",
    "    'metallic', 'glitter', 'creamy', 'whipped', 'gel', 'liquid', 'cream',\n",
    "    'mousse', 'balm', 'oil', 'stick', 'cushion', 'lightweight', 'full-coverage',\n",
    "    \n",
    "    # Ingredientes clave\n",
    "    'retinol', 'vitamin c', 'hyaluronic acid', 'peptides', 'niacinamide', 'aha',\n",
    "    'bha', 'salicylic acid', 'glycolic acid', 'lactic acid', 'ceramides', 'spf',\n",
    "    'antioxidants', 'collagen', 'aloe', 'shea butter', 'tea tree', 'squalane',\n",
    "    'bakuchiol', 'vitamin e', 'zinc', 'caffeine', 'centella', 'propolis',\n",
    "    \n",
    "    # Características y beneficios\n",
    "    'vegan', 'cruelty-free', 'clean', 'organic', 'natural', 'dermatologist-tested',\n",
    "    'hypoallergenic', 'non-comedogenic', 'paraben-free', 'sulfate-free', 'silicone-free',\n",
    "    'fragrance-free', 'alcohol-free', 'gluten-free', 'oil-free', 'waterproof',\n",
    "    'longwear', 'transfer-proof', 'buildable', 'hydrating', 'moisturizing', 'brightening',\n",
    "    'anti-aging', 'firming', 'plumping', 'exfoliating', 'soothing', 'calming',\n",
    "    \n",
    "    # Estilo y mercado\n",
    "    'luxury', 'premium', 'affordable', 'professional', 'korean', 'japanese', 'french',\n",
    "    'k-beauty', 'j-beauty', 'inclusive', 'sustainable', 'refillable', 'travel-sized',\n",
    "    'limited-edition', 'customizable', 'unisex', 'cruelty free'\n",
    "}\n",
    "\n",
    "# Frases a excluir del formato de descripción de Sephora (convertido a conjunto)\n",
    "sephora_format_phrases = {\n",
    "    'what it is', 'what else you need to know', 'what else you need', 'what else', \n",
    "    'you need to know', 'you need to', 'need to know', 'need to', 'to know',\n",
    "    'it is', 'it is a', 'is a', 'this is', 'this is a', 'that is', 'that is a',\n",
    "    'else you need', 'else you', 'with a', 'without a', 'of the', 'in the', 'on the',\n",
    "    'for the', 'at the', 'by the', 'from the', 'to the', 'with the', 'and the',\n",
    "    'highlighted ingredients', 'ingredient callouts', 'formulation type', 'coverage',\n",
    "    'finish', 'benefits', 'formulation', 'why we like it', 'how to use', 'what you get',\n",
    "    'key ingredients', 'key benefits', 'suggested usage', 'free of', 'made with',\n",
    "    'contains', 'includes', 'helps', 'helps to', 'designed to', 'works to',\n",
    "    'makes', 'gives', 'provides', 'delivers', 'offers', 'features', 'perfect for',\n",
    "    'ideal for', 'great for', 'suitable for', 'specially formulated',\n",
    "    'clinically proven', 'dermatologist tested', 'ophthalmologist tested'\n",
    "}\n",
    "\n",
    "# Palabras genéricas a excluir (convertido a conjunto)\n",
    "generic_words = {\n",
    "    'it', 'is', 'it is', 'this', 'this is', 'that', 'that is', 'the', 'and', 'are', 'these', \n",
    "    'those', 'with', 'without', 'synthetic', 'else you need', 'else you', 'you need to', \n",
    "    'need to know', 'need to', 'to know', 'what it is', 'is a', 'what else you', 'what else', \n",
    "    'help', 'contains', 'it is a', 'from', 'into', 'during', 'before', 'after', 'above', \n",
    "    'below', 'between', 'among', 'throughout', 'through', 'over', 'under', 'within', 'along', \n",
    "    'across', 'around', 'about', 'against', 'beyond', 'near', 'same', 'different', 'various',\n",
    "    'several', 'few', 'many', 'much', 'some', 'any', 'all', 'every', 'each', 'either',\n",
    "    'neither', 'both', 'such', 'like', 'unlike', 'similar', 'other', 'another',\n",
    "    'than', 'then', 'now', 'later', 'earlier', 'soon', 'already', 'still', 'yet',\n",
    "    'once', 'twice', 'again', 'ever', 'never', 'always', 'often', 'seldom', 'usually',\n",
    "    'generally', 'sometimes', 'rarely', 'frequently', 'occasionally', 'normally',\n",
    "    'commonly', 'naturally', 'especially', 'particularly', 'specifically', 'exactly',\n",
    "    'precisely', 'approximately', 'roughly', 'nearly', 'almost', 'virtually', 'practically',\n",
    "    'essentially', 'basically', 'fundamentally', 'primarily', 'mainly', 'mostly',\n",
    "    'largely', 'chiefly', 'principally', 'predominantly', 'significantly',\n",
    "    'notably', 'markedly', 'considerably', 'substantially', 'extensively', 'thoroughly',\n",
    "    'completely', 'entirely', 'totally', 'wholly', 'fully', 'partly', 'partially',\n",
    "    'somewhat', 'slightly', 'relatively', 'comparatively', 'rather', 'quite', 'extremely',\n",
    "    'very', 'too', 'enough', 'sufficiently', 'adequately', 'properly', 'correctly',\n",
    "    'accordingly', 'consequently', 'therefore', 'thus', 'hence', 'so', 'otherwise',\n",
    "    'nonetheless', 'nevertheless', 'however', 'although', 'though', 'even', 'just',\n",
    "    'only', 'merely', 'simply', 'really', 'actually', 'fact', 'indeed', 'certainly',\n",
    "    'definitely', 'absolutely', 'undoubtedly', 'surely', 'clearly', 'obviously',\n",
    "    'apparently', 'seemingly', 'possibly', 'perhaps', 'maybe', 'probably',\n",
    "    'likely', 'unlikely', 'doubtfully', 'supposedly', 'allegedly', 'reportedly',\n",
    "    'furthermore', 'moreover', 'additionally', 'also', 'besides',\n",
    "    'likewise', 'similarly', 'instead', 'alternatively', 'conversely',\n",
    "    'whereas', 'while', 'because', 'since', 'due', 'owing', 'result',\n",
    "    'subsequently', 'next', 'last', 'finally', 'ultimately', 'eventually', \n",
    "    'steadily', 'continuously', 'constantly', 'persistently', 'consistently', 'regularly',\n",
    "    'periodically', 'intermittently', 'sporadically', 'randomly', 'suddenly', 'abruptly',\n",
    "    'immediately', 'instantly', 'quickly', 'rapidly', 'swiftly', 'promptly', 'hastily',\n",
    "    'speedily', 'slowly', 'deliberately', 'purposefully', 'intentionally',\n",
    "    'knowingly', 'unwittingly', 'accidentally', 'mistakenly', 'erroneously',\n",
    "    'incorrectly', 'rightly', 'appropriately', 'suitably',\n",
    "    'satisfactorily', 'effectively', 'efficiently', 'successfully', 'poorly', 'badly',\n",
    "    'terribly', 'horribly', 'awfully', 'dreadfully', 'wonderfully', 'beautifully',\n",
    "    'nicely', 'pleasantly', 'delightfully', 'fortunately', 'unfortunately', 'happily',\n",
    "    'sadly', 'regrettably', 'hopefully', 'presumably',\n",
    "    'evidently', 'unquestionably', 'indisputably', 'indubitably', 'no', 'not', 'none', \n",
    "    'nobody', 'nothing', 'nowhere', 'nor', 'one', 'everything', 'everyone', 'everybody',\n",
    "    'everywhere', 'both', 'someone', 'somebody', 'something', 'somewhere', 'sometime', \n",
    "    'anyone', 'anybody', 'anything', 'anywhere', 'anytime',\n",
    "    \n",
    "    # Palabras específicas del dominio que queremos excluir\n",
    "    'is', 'application', 'collection', 'bristles', 'know', 'skin', 'brushes', \"what\",\"liquid\", 'lips', 'brow', 'finish', 'brows', 'formula', 'color',\n",
    "    'reflects', 'shades', 'parabens',\n",
    "    'brand', 'product', 'products', 'item', 'items', 'use', 'using', 'used',\n",
    "    'look', 'looks', 'looking', 'apply', 'applied', 'applying', 'wear', 'wearing',\n",
    "    'choose', 'time', 'times', 'day', 'days', 'week', 'weeks', 'month', 'months',\n",
    "    'year', 'years', 'size', 'small', 'medium', 'large', 'mini', 'full',\n",
    "    'new', 'old', 'fresh', 'set', 'sets', 'kit', 'kits', 'bundle', 'feature',\n",
    "    'features', 'benefit', 'benefits', 'include', 'includes', 'included',\n",
    "    'contain', 'contains', 'contained', 'create', 'creates', 'created',\n",
    "    'provide', 'provides', 'provided', 'offer', 'offers', 'offered',\n",
    "    'exclusive', 'recommend', 'recommended', 'favorite', 'favorites',\n",
    "    'best', 'better', 'good', 'great', 'excellent', 'amazing', 'perfect',\n",
    "    'love', 'loved', 'loves', 'loving', 'like', 'liked', 'likes', 'liking',\n",
    "    'works', 'worked', 'working', 'want', 'wanted', 'wanting', 'wants',\n",
    "    'designed', 'designed for', 'specially', 'special', 'specific', 'specifically',\n",
    "    'designed to', 'made to', 'formulated', 'formulated to', 'specially formulated'\n",
    "}\n",
    "\n",
    "def extract_meaningful_terms(text, target_terms=cosmetic_specific_terms, \n",
    "                             excluded_phrases=sephora_format_phrases,\n",
    "                             generic_stopwords=generic_words):\n",
    "    \"\"\"\n",
    "    Extrae términos significativos y representativos específicos del sector cosmético,\n",
    "    evitando frases del formato de descripción de Sephora y palabras genéricas.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Convertir a minúsculas y limpieza inicial\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s-]', ' ', text)  # Reemplazar puntuación conservando guiones\n",
    "    text = re.sub(r'\\s+', ' ', text)       # Normalizar espacios\n",
    "    \n",
    "    # Preprocesamiento: eliminar frases del formato de descripciones de Sephora\n",
    "    processed_text = ' ' + text + ' '  # Añadir espacios para detectar palabras completas\n",
    "    for phrase in excluded_phrases:\n",
    "        processed_text = processed_text.replace(' ' + phrase + ' ', ' ')\n",
    "    \n",
    "    # Volver a normalizar espacios después de eliminar frases\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
    "    \n",
    "    # Buscar términos específicos de cosmética en el texto procesado\n",
    "    found_terms = []\n",
    "    \n",
    "    # Primero buscar términos específicos de cosmética (prioridad)\n",
    "    for term in target_terms:\n",
    "        # Verificar si el término aparece como palabra completa\n",
    "        pattern = r'\\b' + re.escape(term) + r'\\b'\n",
    "        if re.search(pattern, processed_text):\n",
    "            # Verificar que ninguna palabra del término esté en la lista de stopwords\n",
    "            term_words = term.split()\n",
    "            if not any(word in generic_stopwords for word in term_words):\n",
    "                found_terms.append(term)\n",
    "    \n",
    "    # Buscar palabras individuales que podrían ser relevantes\n",
    "    individual_words = set(re.findall(r'\\b[a-z]{3,}\\b', processed_text))\n",
    "    \n",
    "    # Filtrar palabras individuales contra stopwords\n",
    "    for word in individual_words:\n",
    "        if (word not in generic_stopwords and \n",
    "            word not in found_terms and \n",
    "            not any(word in term.split() for term in found_terms)):\n",
    "            found_terms.append(word)\n",
    "    \n",
    "    # Contar frecuencias de los términos encontrados\n",
    "    term_counts = Counter(found_terms)\n",
    "    \n",
    "    # Filtro final: verificar una vez más que ningún término contenga palabras en generic_stopwords\n",
    "    filtered_terms = Counter()\n",
    "    for term, count in term_counts.items():\n",
    "        term_words = term.split()\n",
    "        if not any(word in generic_stopwords for word in term_words):\n",
    "            filtered_terms[term] = count\n",
    "    \n",
    "    return filtered_terms\n",
    "\n",
    "# Preparar el dataframe\n",
    "print(\"Procesando datos y extrayendo términos significativos por marca...\")\n",
    "\n",
    "# Combinar columnas de descripción\n",
    "description_columns = [col for col in ['description', 'What it is', 'Finish', 'What Else You Need to Know'] \n",
    "                     if col in sephora_df.columns]\n",
    "\n",
    "# Crear una columna combinada de descripciones\n",
    "sephora_df['combined_description'] = sephora_df[description_columns].apply(\n",
    "    lambda row: ' '.join([str(cell) for cell in row if not pd.isna(cell)]), axis=1\n",
    ")\n",
    "\n",
    "# Agrupar por marca\n",
    "brand_groups = sephora_df.groupby('brand')\n",
    "\n",
    "# Lista para almacenar los resultados\n",
    "brand_data = []\n",
    "\n",
    "# Procesar cada marca\n",
    "for brand, group in brand_groups:\n",
    "    # Combinar todas las descripciones de esta marca\n",
    "    all_descriptions = ' '.join(group['combined_description'].tolist())\n",
    "    \n",
    "    # Extraer términos significativos\n",
    "    term_counts = extract_meaningful_terms(all_descriptions)\n",
    "    \n",
    "    # Obtener los 30 términos más frecuentes\n",
    "    top_terms = [term for term, count in term_counts.most_common(30)]\n",
    "    \n",
    "    # Calcular el precio promedio de los productos de esta marca\n",
    "    avg_price = group['current_price'].astype(float).mean() if 'current_price' in group.columns else np.nan\n",
    "    \n",
    "    # Contar número de productos de esta marca\n",
    "    num_products = len(group)\n",
    "    \n",
    "    # Guardar datos de esta marca\n",
    "    brand_data.append({\n",
    "        'brand': brand,\n",
    "        'representative_terms': ', '.join(top_terms),\n",
    "        'avg_price': avg_price,\n",
    "        'num_products': num_products\n",
    "    })\n",
    "\n",
    "# Crear dataframe con la información por marca\n",
    "brand_df = pd.DataFrame(brand_data)\n",
    "\n",
    "# Ordenar por número de productos (marcas con más productos primero)\n",
    "brand_df = brand_df.sort_values('num_products', ascending=False)\n",
    "\n",
    "# Guardar en Excel\n",
    "output_file = os.path.join(output_dir, 'brand_meaningful_terms_forkmeans.xlsx')\n",
    "brand_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: {output_file}\")\n",
    "print(f\"Total de marcas procesadas: {len(brand_df)}\")\n",
    "\n",
    "# Mostrar algunos ejemplos\n",
    "print(\"\\nEjemplos de las primeras marcas procesadas:\")\n",
    "for i, row in brand_df.head(3).iterrows():\n",
    "    print(f\"\\nMarca: {row['brand']}\")\n",
    "    print(f\"Promedio de precio: ${row['avg_price']:.2f}\")\n",
    "    print(f\"Número de productos: {row['num_products']}\")\n",
    "    print(f\"Términos significativos: {row['representative_terms']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
